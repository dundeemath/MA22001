[
  {
    "objectID": "volumes.html",
    "href": "volumes.html",
    "title": "Volumes in three dimensions",
    "section": "",
    "text": "This part of the module deals with sets in 3D.1\nSets are defined by equalities and inequalities. The simplest type we will see are those sets which are defined only by inequalities and no equalities. For example, consider the set \\(S\\) where \\[\nx\\geq 0.\n\\] We haven’t mentioned \\(y\\) or \\(z\\), so we assume they can take any values. Formally, we might write this as2 \\[\nS = \\{(x,y,z)\\in\\mathbb{R}^3\\,|\\,x\\geq 0\\}.\n\\] This is an infinite volume, it’s all the points on one side of the \\(y-z\\) plane.\nWhen we impose an inequality, we restrict a given set to a smaller region of the same dimension. If we instead we impose an equality, we reduce the dimension, as we shall see next time.",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Volumes"
    ]
  },
  {
    "objectID": "volumes.html#volumes-in-curvilinear-co-ordinates",
    "href": "volumes.html#volumes-in-curvilinear-co-ordinates",
    "title": "Volumes in three dimensions",
    "section": "Volumes in curvilinear co-ordinates",
    "text": "Volumes in curvilinear co-ordinates\nIt’s often useful to describe volumes in curvilinear co-ordinates, for example spherical or cylindrical co-ordinates. The inequalities will then be in terms of the curvilinear co-ordinates.\n\nExample\nDescribe the set given in spherical co-ordinates by \\[\nB = \\{(r,\\theta,\\phi)\\,|\\,0\\leq r&lt;\\infty,\\,0\\leq \\theta &lt; 2\\pi,\\,0\\leq \\phi \\leq \\pi/4\\}.\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis set is defined by three inequalities but no equalities, so it’s a volume.\nIt’s the inside of an infinite cone around the \\(z\\)-axis, with half-angle \\(\\pi/4\\).\n\n\n\n\n\nExample\nChoose a better parameterisation for the volume \\[\nC = \\{(x,y,z)\\in\\mathbb{R}^3\\,|\\,x^2+y^2+z^2\\leq a^2,\\,z\\geq 0\\}.\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe volume \\(C\\) is the upper half of a ball of radius \\(a\\) centred at the origin. A better parameterisation is in spherical coordinates: \\[\nC = \\{(r,\\theta,\\phi)\\,|\\,0\\leq r\\leq a,\\,0\\leq \\theta &lt; 2\\pi,\\,0\\leq \\phi \\leq \\pi/2\\}.\n\\]",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Volumes"
    ]
  },
  {
    "objectID": "volumes.html#footnotes",
    "href": "volumes.html#footnotes",
    "title": "Volumes in three dimensions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd specifically, ‘nice’ sets which can be differentiated and integrated. We won’t go too much into the mathematical details, but these are called manifolds. This excludes things like fractal sets.↩︎\nRead this as “S is the set of x y z in R three such that x is non-negative”↩︎",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Volumes"
    ]
  },
  {
    "objectID": "vectorfields.html",
    "href": "vectorfields.html",
    "title": "Derivatives of vector fields",
    "section": "",
    "text": "Another thing that changes when moving from 1D to 3D is that we now have a choice of two types of functions, scalar fields or vector fields. Now we’ll think about vector fields, that is, assigning a magnitude and a direction to every point in space. Think about those wind maps on the weather forecast, or magnetic field lines around a magnet.\nTo know a vector field, we need to know each of its components at each point in space. \\[\n{\\mathbf{F}}(x,y,z) = F_1(x,y,z){\\mathbf{e}}_x + F_2(x,y,z){\\mathbf{e}}_y + F_3(x,y,z){\\mathbf{e}}_z,\n\\] or equivalently in a different co-ordinate system.\nFirstly, given any vector field we can easily construct a scalar field by taking the magnitude at each point in space, \\[\n\\left|{\\mathbf{F}}(x,y,z)\\right|=\\sqrt{F_1(x,y,z)^2 + F_2(x,y,z)^2 + F_3(x,y,z)^2}\n\\] and then take scalar derivatives.\nWe consider two different types.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Derivatives of vector fields"
    ]
  },
  {
    "objectID": "vectorfields.html#footnotes",
    "href": "vectorfields.html#footnotes",
    "title": "Derivatives of vector fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHow can you prove this?↩︎",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Derivatives of vector fields"
    ]
  },
  {
    "objectID": "surface.html",
    "href": "surface.html",
    "title": "Surfaces",
    "section": "",
    "text": "Curves in 3D can be seen as a generalisation of curves in 2D, but a different generalisation would be surfaces in 3D1. The easiest way to write down a surface is \\[\nz = f(x,y).\n\\] This is exactly equivalent to \\(y=f(x)\\) in 2D, and you can think of it as giving the height of the surface at any points \\(x\\) and \\(y\\).\nActually, any single equation (an equality constraint) relating the variables \\(x\\), \\(y\\) and \\(z\\) will in general describe a surface in 3D. For example, \\[\nx^2 + y^2 + z^2 = 1\n\\] is the sphere of radius 1 centred on the origin (MA12001), and \\[\nx = 1\n\\] is the plane parallel to the \\(y-z\\) plane which goes through the point \\([1,0,0]\\).",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Surfaces"
    ]
  },
  {
    "objectID": "surface.html#footnotes",
    "href": "surface.html#footnotes",
    "title": "Surfaces",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCurves have dimension 1. But a curve in 2D has co-dimension 1 and a curve in 3D has co-dimension 2, whereas a surface in 3D has co-dimension 1. The co-dimension of a subset is the dimension of the space minus the dimension of the subset. The co-dimension is the number of equations you need to describe a subset.↩︎\nIn the next unit, we’ll write this as \\({\\mathbf{n}} = \\nabla f\\).↩︎",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Surfaces"
    ]
  },
  {
    "objectID": "separationofvariables.html",
    "href": "separationofvariables.html",
    "title": "Separation of variables for PDEs",
    "section": "",
    "text": "You have already seen a method called separation of variables for solving ordinary differential equations (ODEs). Another method, with the same name, but almost completely unrelated, is used to find solutions to certain partial differential equations (PDEs). The basic idea is to guess the form of the solution as a product of functions, each depending on only one variable.\nFor example, if we have a PDE for \\(u(x,t)\\) in two variables \\(x\\) and \\(t\\), we might guess that the solution has the form \\[\nu(x,t) = X(x)T(t),\n\\] where \\(X(x)\\) is a function of \\(x\\) alone, and \\(T(t)\\) is a function of \\(t\\) alone. We can then substitute this guess into the PDE and see if we can separate the variables so that we get two ordinary differential equations, one for \\(X(x)\\) and one for \\(T(t)\\).",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Separation of variables"
    ]
  },
  {
    "objectID": "separationofvariables.html#a-basic-example",
    "href": "separationofvariables.html#a-basic-example",
    "title": "Separation of variables for PDEs",
    "section": "A basic example",
    "text": "A basic example\nSuppose that we want to solve the one-dimensional heat equation \\[\n{\\frac{\\partial u}{\\partial t}} = {\\frac{\\partial^2 u}{\\partial x^2}},\n\\] subject to boundary conditions \\(u(0,t) = u(\\pi,t) = 0\\) for all \\(t\\)1, and an initial condition \\(u(x,0) = \\sin x\\).\nWe try separation of variables by guessing \\[\nu(x,t) = X(x)T(t).\n\\]\nSubstituting into the PDE gives \\[\nX(x)T'(t) = T(t)X''(x),\n\\] where the prime denotes differentiation with respect to the relevant variable.\nIf \\(X(x)\\neq 0\\) and \\(T(t)\\neq 0\\) we can divide both sides by \\(X(x)T(t)\\) to get \\[\n\\frac{T'(t)}{T(t)} = \\frac{X''(x)}{X(x)}.\n\\]\nThe left-hand side depends only on \\(t\\) and the right-hand side depends only on \\(x\\), so both sides must be equal to the same constant. It is convenient to call this constant \\(-\\lambda\\): \\[\n\\frac{T'(t)}{T(t)} = \\frac{X''(x)}{X(x)} = -\\lambda.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nThis is the crucial step!\n\n\nThis gives two ODEs: \\[\n\\begin{aligned}\nT'(t) + \\lambda T(t) &= 0,\\\\\nX''(x) + \\lambda X(x) &= 0.\n\\end{aligned}\n\\]\nThe first equation has solution \\[\nT(t)=Ae^{-\\lambda t}.\n\\]\nFor \\(X\\) we must consider different signs of \\(\\lambda\\), and (crucially) we must now impose the boundary conditions.\n\nUsing the boundary conditions to determine \\(\\lambda\\)\n\nCase \\(\\lambda = 0\\). Then \\(X''=0\\), so \\(X(x)=ax+b\\). The boundary condition \\(X(0)=0\\) gives \\(b=0\\), and \\(X(\\pi)=0\\) gives \\(a=0\\). Hence \\(X\\equiv 0\\), so this produces only the trivial solution \\(u\\equiv 0\\).\nCase \\(\\lambda&lt;0\\). Write \\(\\lambda=-\\mu^2\\) with \\(\\mu&gt;0\\). Then \\(X''-\\mu^2 X=0\\), so \\[\nX(x)=Be^{\\mu x}+Ce^{-\\mu x}.\n\\] Imposing \\(X(0)=0\\) gives \\(B+C=0\\), so \\(X(x)=B(e^{\\mu x}-e^{-\\mu x})\\). Then \\(X(\\pi)=0\\) forces \\(B=0\\), again giving only the trivial solution.\nCase \\(\\lambda&gt;0\\). Write \\(\\lambda=\\mu^2\\) with \\(\\mu&gt;0\\). Then \\[\nX(x)=B\\sin(\\mu x)+C\\cos(\\mu x).\n\\] Imposing \\(X(0)=0\\) gives \\(C=0\\), so \\(X(x)=B\\sin(\\mu x)\\). Imposing \\(X(\\pi)=0\\) then gives \\[\n\\sin(\\mu\\pi)=0 \\quad\\Longrightarrow\\quad \\mu = n \\in \\mathbb{N}.\n\\] Hence \\[\n\\lambda = n^2,\\qquad X_n(x)=\\sin(nx).\n\\]\n\nSo the separated solutions that satisfy the boundary conditions are \\[\nu_n(x,t)=e^{-n^2 t}\\sin(nx),\\qquad n=1,2,3,\\dots\n\\] (Here we have absorbed any overall constant into the coefficient in front of \\(\\sin(nx)\\).)\n\n\nMatching the initial condition\nFor the initial condition \\(u(x,0)=\\sin x\\), we simply take \\(n=1\\), giving \\[\nu(x,t)=e^{-t}\\sin x,\n\\] which satisfies the PDE, the boundary conditions, and the initial condition.\n\n\nDifferent initial conditions\nNow suppose the initial condition is \\[\nu(x,0)=2\\sin(2x)-\\sin(3x).\n\\] We use the corresponding separated solutions \\(u_2\\) and \\(u_3\\): \\[\nu_2(x,t)=e^{-4t}\\sin(2x),\\qquad u_3(x,t)=e^{-9t}\\sin(3x).\n\\] Because the heat equation is linear, any linear combination of solutions is also a solution, so \\[\nu(x,t)=2e^{-4t}\\sin(2x)-e^{-9t}\\sin(3x)\n\\] solves the PDE and boundary conditions, and it matches the initial condition at \\(t=0\\).",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Separation of variables"
    ]
  },
  {
    "objectID": "separationofvariables.html#mixed-partial-derivatives",
    "href": "separationofvariables.html#mixed-partial-derivatives",
    "title": "Separation of variables for PDEs",
    "section": "Mixed partial derivatives",
    "text": "Mixed partial derivatives\nConsider the PDE \\[ {\\frac{\\partial^2 u}{\\partial x^2}} + {\\frac{\\partial^2 u}{\\partial y^2}} - \\frac{\\partial^2 u}{\\partial x \\partial y} = 0. \\] If we try the separation of variables method by guessing that the solution has the form \\(u(x,y) = X(x)Y(y)\\), then we get \\[ X''(x)Y(y) + X(x)Y''(y) - X'(x)Y'(y) = 0. \\] If \\(X(x) \\neq 0\\) and \\(Y(y) \\neq 0\\), then we can divide through by \\(X(x)Y(y)\\) and rearrange to get \\[ \\frac{X''(x)}{X(x)} + \\frac{Y''(y)}{Y(y)} - \\frac{X'(x)}{X(x)} \\frac{Y'(y)}{Y(y)} = 0. \\] We cannot easily separate the variables in this case. However, if we differentiate this equation with respect to \\(x\\), we get \\[ \\frac{X'''(x)X(x) -X''(x)X'(x)}{X(x)^2} - \\frac{X''(x)X(x) -X'(x)X'(x)}{X(x)^2}\\frac{Y'(y)}{Y(y)} = 0, \\] which can be rearranged to get \\[ \\frac{X'''(x)X(x) -X''(x)X'(x)}{X''(x)X(x) -X'(x)X'(x)} = \\frac{Y'(y)}{Y(y)}. \\] This is now separated, and we can proceed as before to get two ODEs that we can solve to get a solution to the original PDE. However, the resulting ODE for \\(X(x)\\) is quite complicated.2",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Separation of variables"
    ]
  },
  {
    "objectID": "separationofvariables.html#nonlinear-pdes",
    "href": "separationofvariables.html#nonlinear-pdes",
    "title": "Separation of variables for PDEs",
    "section": "Nonlinear PDEs",
    "text": "Nonlinear PDEs\nThe examples above are linear PDEs, so sums of solutions are still solutions. Occasionally, separation of variables can also produce special solutions for nonlinear PDEs, but superposition does not work in general.\nConsider the inviscid Burgers’ equation \\[\n{\\frac{\\partial u}{\\partial t}} + u {\\frac{\\partial u}{\\partial x}} = 0.\n\\] Try \\(u(x,t)=X(x)T(t)\\). Then \\[\nu_t=X T',\\qquad u_x=X'T,\\qquad u\\,u_x=(XT)(X'T)=XX' T^2,\n\\] so the PDE becomes \\[\nX(x)T'(t) + X(x)X'(x)T(t)^2 = 0.\n\\] If \\(X\\neq 0\\) and \\(T\\neq 0\\), divide by \\(X(x)T(t)^2\\): \\[\n\\frac{T'(t)}{T(t)^2} = -X'(x).\n\\] As usual, one side depends only on \\(t\\) and the other only on \\(x\\), so both must be equal to a constant, say \\(\\lambda\\): \\[\n\\frac{T'(t)}{T(t)^2} = \\lambda,\\qquad X'(x) = -\\lambda.\n\\]\nThese ODEs can be solved explicitly: - From \\(X'=-\\lambda\\), we get \\(X(x)=-\\lambda x + B\\). - From \\(T'/T^2=\\lambda\\), we get \\(-1/T = \\lambda t + C\\), so \\[\n  T(t)= -\\frac{1}{\\lambda t + C}.\n  \\]\nPutting this together (for \\(\\lambda\\neq 0\\)), \\[\nu(x,t)=X(x)T(t)=\\frac{\\lambda x - B}{\\lambda t + C}.\n\\] (There is also the \\(\\lambda=0\\) case, which gives constant solutions \\(u\\equiv \\text{constant}\\).)\n\n\n\n\n\n\nNote\n\n\n\nThese are special separated solutions. The general solution of inviscid Burgers’ equation is usually found using the method of characteristics, and solutions can develop shocks (discontinuities) even from smooth initial data.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Separation of variables"
    ]
  },
  {
    "objectID": "separationofvariables.html#footnotes",
    "href": "separationofvariables.html#footnotes",
    "title": "Separation of variables for PDEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese boundary conditions mean that the temperature is fixed at 0 at both ends of the rod. We could also consider boundary conditions where the first derivative is fixed, e.g. \\({\\frac{\\partial u}{\\partial x}}(0,t) = {\\frac{\\partial u}{\\partial x}}(L,t) = 0\\), which mean that no heat is flowing in or out at the ends of the rod.↩︎\nThis is just an example to show that sometimes you can still use separation of variables even if the PDE has mixed partial derivatives, but it is not a method that I would recommend in general. In this case, we could change variables to \\(u(x,y)=v(x+y,x-y)\\), which would give a much simpler PDE for \\(v\\), which we could then solve using separation of variables.↩︎",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Separation of variables"
    ]
  },
  {
    "objectID": "scalarfields.html",
    "href": "scalarfields.html",
    "title": "Derivatives of scalar fields",
    "section": "",
    "text": "In this part of the module, we study how to differentiate fields. A field is another word for a function defined over space. Starting with scalar fields: assigning to each point in space a scalar value. Think about a temperature field.\nFor example, the following could be a scalar field \\[\nf({\\mathbf{r}}) = f(x,y,z) = x^2y +z.\n\\] If you know a point in space (given by co-ordinates or a position vector) then you know the value of \\(f\\) at that point.\nThere are different types of differentiation in higher dimensions, and they are generalisations of different interpretations of the normal derivative.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Derivatives of scalar fields"
    ]
  },
  {
    "objectID": "scalarfields.html#footnotes",
    "href": "scalarfields.html#footnotes",
    "title": "Derivatives of scalar fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe already saw this for calculating normals to surfaces.↩︎\nStrictly speaking the symbol \\(\\nabla\\) is called “nabla” or “del”, but we often call it “grad” in other settings.↩︎\nIn fact, we could see straight away that \\[\n\\nabla f \\neq {\\frac{\\partial f}{\\partial r}}{\\mathbf{e}}_r + {\\frac{\\partial f}{\\partial \\theta}}{\\mathbf{e}}_\\theta + {\\frac{\\partial f}{\\partial \\phi}}{\\mathbf{e}}_\\phi\n\\] because of the dimensions of the quantities. The dimensions of a derivative should be the dimensions of \\(f\\) divided by length, which is what the first expression has. But \\(\\theta\\) and \\(\\phi\\) are dimensionless, so we need to divide by a length to keep these ones consistent.↩︎",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Derivatives of scalar fields"
    ]
  },
  {
    "objectID": "pdes.html",
    "href": "pdes.html",
    "title": "Examples of PDEs",
    "section": "",
    "text": "}\nPDEs are partial differential equations. These are equations which relate partial derivatives of a function with each other. Unlike ordinary differential equations (ODEs), which involve ordinary derivatives with respect to a single independent variable, PDEs involve functions of multiple independent variables and their partial derivatives.\nTwo very famous examples of PDEs that are found in physics are the Navier-Stokes equations (which describe fluid flow) \\[\n\\begin{aligned}\n\\frac{\\partial {\\mathbf{u}}}{\\partial t} + ({\\mathbf{u}}\\cdot\\nabla){\\mathbf{u}} &= -\\frac{1}{\\rho}\\nabla p + \\nu \\nabla^2 {\\mathbf{u}} + {\\mathbf{f}},\\\\\n\\nabla \\cdot {\\mathbf{u}} &= 0\n\\end{aligned}\n\\]\nand Maxwell’s equations (which describe electromagnetism): \\[\n\\begin{aligned}\n\\nabla \\cdot {\\mathbf{E}} &= \\frac{\\rho}{\\epsilon_0} \\quad\\text{(Gauss's law)} \\\\\n\\nabla \\cdot {\\mathbf{B}} &= 0 \\quad\\text{(No magnetic monopoles)} \\\\\n\\nabla \\times {\\mathbf{E}} &= -\\frac{\\partial {\\mathbf{B}}}{\\partial t} \\quad\\text{(Faraday's law)} \\\\\n\\nabla \\times {\\mathbf{B}} &= \\mu_0\\left({\\mathbf{J}} + \\epsilon_0\\frac{\\partial {\\mathbf{E}}}{\\partial t}\\right) \\quad\\text{(Ampère-Maxwell law)}\n\\end{aligned}\n\\]\nEach of these PDEs needs a whole module of its own, but we will see some very simple examples here.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Week 20)",
      "Examples of PDEs"
    ]
  },
  {
    "objectID": "pdes.html#existence-and-uniqueness-of-solutions",
    "href": "pdes.html#existence-and-uniqueness-of-solutions",
    "title": "Examples of PDEs",
    "section": "Existence and uniqueness of solutions",
    "text": "Existence and uniqueness of solutions\nFor many PDEs, it is not always clear whether a solution exists, or whether it is unique. This is a very important question, because if there are multiple solutions, then we need some way to determine which one is the ‘correct’ solution for a given physical situation. For linear PDEs, there are often well-established methods for proving existence and uniqueness of solutions. For non-linear PDEs, this is much harder, and in many cases it is still an open question whether solutions exist or are unique.\nFor example, the Navier-Stokes equations are known to have solutions, but it is not known whether these solutions are always smooth (i.e. free of singularities) or whether they can develop singularities in finite time. This is one of the most famous open problems in mathematics, and it is one of the seven Millennium Prize Problems, worth one million dollars.\n\nExample\nFor the linear PDE \\[\n{\\frac{\\partial u}{\\partial t}} = \\alpha {\\frac{\\partial^2 u}{\\partial x^2}},\n\\] subject to the boundary conditions \\(u(0,t) = u(L,t) = 0\\) for all \\(t\\), and the initial condition \\(u(x,0) = f(x)\\) for some function \\(f(x)\\), show that the solution is unique.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSuppose there are two solutions \\(u_1(x,t)\\) and \\(u_2(x,t)\\) to the PDE with the same boundary and initial conditions. Then the difference \\(v(x,t) = u_1(x,t) - u_2(x,t)\\) satisfies the PDE \\[\n{\\frac{\\partial v}{\\partial t}} = {\\frac{\\partial u_1}{\\partial t}} - {\\frac{\\partial u_2}{\\partial t}} = \\alpha {\\frac{\\partial^2 u_1}{\\partial x^2}} - \\alpha {\\frac{\\partial^2 u_2}{\\partial x^2}} = \\alpha {\\frac{\\partial^2 v}{\\partial x^2}},\n\\] with the boundary conditions \\(v(0,t) = u_1(0,t) - u_2(0,t) = 0\\) and \\(v(L,t) = u_1(L,t) - u_2(L,t) = 0\\), and the initial condition \\(v(x,0) = u_1(x,0) - u_2(x,0) = f(x) - f(x) = 0\\).\nSo if we can show that the only solution to this PDE with these boundary and initial conditions is the trivial solution \\(v(x,t) = 0\\), then we will have shown that \\(u_1(x,t) = u_2(x,t)\\), and hence the solution is unique.\nTo complete this proof, we can use the method of separation of variables, which we will see in the next section.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Week 20)",
      "Examples of PDEs"
    ]
  },
  {
    "objectID": "pdes.html#numerical-solutions",
    "href": "pdes.html#numerical-solutions",
    "title": "Examples of PDEs",
    "section": "Numerical solutions",
    "text": "Numerical solutions\nThe main way that PDEs are solved in practice is using numerical methods. These involve discretizing the continuous variables (like space and time) into a grid, and then approximating the derivatives using finite differences or other techniques. This allows us to turn the PDE into a set of algebraic equations that can be solved using a computer.\nThis is how, for example, the weather is predicted, via simulations of the atmosphere using numerical solutions to the Navier-Stokes equations. It is also widely used in financial modelling, engineering simulations, and many other fields.\nWe won’t actually see how to do this in this module, but it’s important to know that this is how most PDEs are solved in practice. In your level 4 projects, you may well use numerical methods to solve PDEs related to your chosen topic.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Week 20)",
      "Examples of PDEs"
    ]
  },
  {
    "objectID": "pdes.html#footnotes",
    "href": "pdes.html#footnotes",
    "title": "Examples of PDEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso called the diffusion equation↩︎\nthey are one-dimensional time-evolution equations featuring a self-advection term↩︎",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Week 20)",
      "Examples of PDEs"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful links",
    "section": "",
    "text": "My notes for the vectors stuff of MA12001 are here: https://dundeemath.github.io/MA12001-algebra/_site/. This module builds directly on this (particularly chapters 1 and 3) so it may be helpful to revise this.\nMost of the important results in this module are here: https://en.wikipedia.org/wiki/Vector_calculus_identities. Your formula sheet in the exam should be based off this.\nThe curvilinear co-ordinate results are summarised here: https://en.wikipedia.org/wiki/Del_in_cylindrical_and_spherical_coordinates.",
    "crumbs": [
      "Helpful links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA22001 Core Mathematics IV",
    "section": "",
    "text": "This webpage contains the lecture notes for the Core Mathematics IV (MA22001) module for the University of Dundee.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-to-use-these-notes",
    "href": "index.html#how-to-use-these-notes",
    "title": "MA22001 Core Mathematics IV",
    "section": "How to use these notes",
    "text": "How to use these notes\nThese notes are designed to be complementary to the seminars. They’re designed to be easy to read with minimal extra information. For context and motivation, you’ll need to come to class. These notes contain the key definitions and some brief examples, but I will show more and different examples in class. For best results, I suggest quickly reading over these notes the night before each class, and then revisiting them if necessary when you come to the problem sheets and before the tests and exam.\nAs usual, I’ll be taking photos of the boards from the seminars in case you want to revisit a specific example we did there, though I don’t promise that you’ll be able to follow these without having attended the seminars.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#what-is-core-mathematics-iv",
    "href": "index.html#what-is-core-mathematics-iv",
    "title": "MA22001 Core Mathematics IV",
    "section": "What is Core Mathematics IV",
    "text": "What is Core Mathematics IV\nThe bulk of this module is “Vector Calculus”. Vector calculus is basically the mathematics of shapes in three dimensions. This will combine the “algebra” and “calculus” that you’ve seen in previous modules. First we’ll see differentiation in 3D, then integration in 3D, and then some analogues of the “fundamental theorem of calculus” which combine these two.\nWe’ll also see some methods for solving differential equations in the second half of the module.\nEverything in this module is fundamental to anyone who wants to study maths or physics2 at a higher level. This builds off previous modules and many future modules will build off this. This is not a physics module, but I will appeal to applications in physics a lot.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "MA22001 Core Mathematics IV",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnything in the footnotes is also non-examinable.↩︎\nor indeed engineering↩︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "fourierseries.html",
    "href": "fourierseries.html",
    "title": "Fourier series",
    "section": "",
    "text": "The Fourier series of a periodic function allows us to express that function as a sum of sines and cosines. As long as a function is exactly periodic, that is \\(f(x + L) = f(x)\\) for some period \\(L\\), and it is “nice” enough (piecewise continuous, etc.), we can represent it as a Fourier series.\nWe write that1 \\[\nf(x) = a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] where the \\(a_n\\) and \\(b_n\\) are the Fourier coefficients.\nConsider the function \\(f(x) = \\sin x\\). This is periodic with period \\(L = 2\\pi\\). It is already in the form of a Fourier series, with coefficients \\[\na_0 = 0, \\quad a_n = 0 \\text{ for all } n, \\quad b_1 = 1, \\quad b_n = 0 \\text{ for } n \\neq 1.\n\\]",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier Series"
    ]
  },
  {
    "objectID": "fourierseries.html#finding-the-fourier-coefficients",
    "href": "fourierseries.html#finding-the-fourier-coefficients",
    "title": "Fourier series",
    "section": "Finding the Fourier coefficients",
    "text": "Finding the Fourier coefficients\nAbove, we saw some very simple examples where we could read off the Fourier coefficients by inspection. In general, we need to use the orthogonality of sines and cosines to find the coefficients. This gives a formula \\[\n\\begin{aligned}\na_n &= \\begin{cases}\n\\frac{1}{L} \\int_0^L f(x)  dx & n = 0\\\\\n\\frac{2}{L} \\int_0^L f(x) \\cos\\left(\\frac{2\\pi n x}{L}\\right) dx & n &gt; 0\n\\end{cases}\\\\\nb_n &= \\frac{2}{L} \\int_0^L f(x)\n\\sin\\left(\\frac{2\\pi n x}{L}\\right) dx\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThis is a very important formula, because it allows us to find the Fourier coefficients for any periodic function, as long as we can evaluate the integrals. In practice, these integrals can often be evaluated using standard techniques of integration, such as substitution, integration by parts, or using known integrals of trigonometric functions.\n\n\n\nProof (Non-examinable)\nUsing the definition of the Fourier series above, we have \\[\n\\begin{aligned}\n\\frac{1}{L} \\int_0^L f(x)  dx &= \\frac{1}{L} \\left\\langle f(x), 1 \\right\\rangle = \\frac{1}{L} \\left\\langle a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right], 1 \\right\\rangle \\\\&= \\frac{a_0}{L}\\left\\langle 1, 1 \\right\\rangle  + \\sum_{n=1}^\\infty \\left[ a_n \\left\\langle \\cos\\left(\\frac{2\\pi n x}{L}\\right), 1 \\right\\rangle + b_n \\left\\langle \\sin\\left(\\frac{2\\pi n x}{L}\\right), 1 \\right\\rangle \\right] \\\\&= a_0.\n\\end{aligned}\n\\] Similarly we have \\[\n\\begin{aligned}\n\\frac{2}{L} \\int_0^L f(x) \\cos\\left(\\frac{2\\pi n x}{L}\\right) dx &= \\frac{2}{L} \\left\\langle f(x), \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\right\\rangle \\\\&= \\frac{2}{L} \\left\\langle a_0 + \\sum_{m=1}^\\infty \\left[ a_m \\cos\\left(\\frac{2\\pi m x}{L}\\right) + b_m \\sin\\left(\\frac{2\\pi m x}{L}\\right) \\right], \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\right\\rangle \\\\&= a_n,\n\\end{aligned}\n\\] and exactly the same with \\(\\sin\\left(\\frac{2\\pi n x}{L}\\right)\\) to get \\(b_n\\).\n\n\nExample\nFind the Fourier coefficients for the triangle wave \\[\nf(x) = \\begin{cases}\n\\frac{2x}{L} & 0 \\leq x &lt; \\frac{L}{2} \\\\\n\\frac{2(L-x)}{L} & \\frac{L}{2} \\leq x \\leq L\n\\end{cases}\n\\] which is periodic with period \\(L\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the constant term, \\[\na_0=\\frac{1}{L}\\int_0^L f(x)\\,dx\n=\\frac{1}{L}\\left(\\int_0^{L/2}\\frac{2x}{L}\\,dx+\\int_{L/2}^L\\frac{2(L-x)}{L}\\,dx\\right)\n=\\frac{1}{2}.\n\\]\nFor \\(n\\ge1\\), \\[\na_n=\\frac{2}{L}\\int_0^L f(x)\\cos\\!\\left(\\frac{2\\pi n x}{L}\\right)\\,dx.\n\\] Using the symmetry \\(f(x)=f(L-x)\\) and \\(\\cos\\!\\left(\\frac{2\\pi n (L-x)}{L}\\right)=\\cos\\!\\left(\\frac{2\\pi n x}{L}\\right)\\), this becomes \\[\na_n=\\frac{4}{L}\\int_0^{L/2}\\frac{2x}{L}\\cos\\!\\left(\\frac{2\\pi n x}{L}\\right)\\,dx\n=\\frac{8}{L^2}\\int_0^{L/2}x\\cos(kx)\\,dx,\n\\quad k=\\frac{2\\pi n}{L}.\n\\] Since \\(\\int x\\cos(kx)\\,dx=\\frac{x\\sin(kx)}{k}+\\frac{\\cos(kx)}{k^2}\\), evaluating from \\(0\\) to \\(L/2\\) gives \\[\n\\int_0^{L/2}x\\cos(kx)\\,dx=\\frac{(-1)^n-1}{k^2},\n\\] and hence \\[\na_n=\\frac{2}{\\pi^2 n^2}\\bigl((-1)^n-1\\bigr)\n=\n\\begin{cases}\n-\\dfrac{4}{\\pi^2 n^2} & n\\ \\text{odd},\\\\[6pt]\n0 & n\\ \\text{even}.\n\\end{cases}\n\\]\nFor the sine coefficients, \\[\nb_n=\\frac{2}{L}\\int_0^L f(x)\\sin\\!\\left(\\frac{2\\pi n x}{L}\\right)\\,dx=0\n\\quad\\text{for all }n,\n\\] again by the symmetry \\(f(x)=f(L-x)\\) and \\(\\sin\\!\\left(\\frac{2\\pi n (L-x)}{L}\\right)=-\\sin\\!\\left(\\frac{2\\pi n x}{L}\\right)\\).\nSo the Fourier series is \\[\nf(x)=\\frac{1}{2}-\\frac{4}{\\pi^2}\\sum_{\\substack{n=1\\\\ n\\ \\text{odd}}}^\\infty\n\\frac{1}{n^2}\\cos\\!\\left(\\frac{2\\pi n x}{L}\\right).\n\\]",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier Series"
    ]
  },
  {
    "objectID": "fourierseries.html#complex-form-of-the-fourier-series",
    "href": "fourierseries.html#complex-form-of-the-fourier-series",
    "title": "Fourier series",
    "section": "Complex form of the Fourier series",
    "text": "Complex form of the Fourier series\nThere are various ways to transform between real trigonometric functions and complex exponentials. Using Euler’s formula, we can rewrite the Fourier series in terms of complex exponentials: \\[\nf(x) = \\sum_{n=-\\infty}^\\infty c_n e^{i \\frac{2\\pi n x}{L}},\n\\] where the complex Fourier coefficients \\(c_n\\) are related to the real coefficients by \\[\nc_0 = a_0,\\quad\nc_n = \\frac{a_n - i b_n}{2}\\ \\text{ for } n&gt;0,\\quad\nc_{-n} = \\frac{a_n + i b_n}{2}\\ \\text{ for } n&gt;0.\n\\] Even if we are dealing with a purely real function, using the complex form can often simplify calculations, especially when dealing with differential equations or integrals, and it is this form that naturally leads us to the Fourier transform when we consider non-periodic functions.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier Series"
    ]
  },
  {
    "objectID": "fields.html",
    "href": "fields.html",
    "title": "Scalar and vector fields",
    "section": "",
    "text": "By this module, you should already be very comfortable with the idea of vectors as things which can be added and multiplied in certain ways, and as having both a direction and a magnitude.\nIn this module, we will deal with vector fields as well as scalar fields. Instead of being a single number, this is a vector or a scalar for every point in space.\nBefore we move onto definitions, here are some examples to make the concepts clear:\nMathematically, we can define a scalar field as a function which takes co-ordinates, and gives a single number. Mostly in this course we will deal with three dimensions, so the function takes three numbers and gives one. We can write this as \\[\n\\alpha : \\mathbb{R}^3 \\to \\mathbb{R}.\n\\] Very often we will use Greek letters for scalar fields.\nA vector field is a function which takes co-ordinates and gives us a vector. In 3D: \\[\n{\\mathbf{u}} : \\mathbb{R}^3 \\to \\mathbb{R}^3.\n\\] When printing, we use bold letters like \\({\\mathbf{u}}\\) for vector fields. You should use an underline when you handwrite them.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Scalar and vector fields"
    ]
  },
  {
    "objectID": "fields.html#making-scalar-fields-from-vector-fields",
    "href": "fields.html#making-scalar-fields-from-vector-fields",
    "title": "Scalar and vector fields",
    "section": "Making scalar fields from vector fields",
    "text": "Making scalar fields from vector fields\nWe’ll soon see ways to differentiate fields which can produce scalar fields from vector fields and vice versa, but even without calculus we can do this.\nFor example, given any two vector fields \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\), we can take the dot product \\({\\mathbf{a}}\\cdot{\\mathbf{b}}\\) which gives a scalar field, or the cross product \\({\\mathbf{a}}\\times{\\mathbf{b}}\\) gives a different vector field. Or we can take the magnitude of a vector field to give a scalar field\n\nExample\nA wind tunnel is decribed by Cartesian co-ordinates with \\(0\\leq x \\leq 1\\), \\(0 \\leq y \\leq 1\\), \\(z\\in \\mathbb{R}\\).1 The velocity of the air in a wind tunnel is given by the vector field \\(\\mathbf{v} = [x(1-x)y(1-y), 0 ,0]\\).2 What is the windspeed, given as a scalar field? At what co-ordinates is the wind fastest? Where is there no wind?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe speed is just the magnitude of the velocity, so we can work out \\[\n|\\mathbf{v}| = \\sqrt{\\left(x(1-x)y(1-y)\\right)^2 + 0^2 + 0^2} = x(1-x)y(1-y).\n\\] Note that this answer is non-negative, as required for a magnitude, because of the range of \\(x\\) and \\(y\\) we are considering.\nThis is greatest when \\(x=1/2\\) and \\(y=1/2\\). The value of \\(z\\) is not important. This is the centre of the wind tunnel.\nThere is no wind when \\(x=0\\) or \\(x=1\\) or \\(y=0\\) or \\(y=1\\). This makes sense - these are the sidewalls of the tunnel.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Scalar and vector fields"
    ]
  },
  {
    "objectID": "fields.html#the-position-vector",
    "href": "fields.html#the-position-vector",
    "title": "Scalar and vector fields",
    "section": "The position vector",
    "text": "The position vector\nA very special vector field is where at each point, we assign the position vector for that point. So at the point \\((x,y,z)\\), the vector field has value \\([x,y,z]\\). We notate this by \\({\\mathbf{r}}\\). So (in Cartesian coordinates, at least) \\[\n{\\mathbf{r}}(x,y,z) = [x,y,z].\n\\]",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Scalar and vector fields"
    ]
  },
  {
    "objectID": "fields.html#footnotes",
    "href": "fields.html#footnotes",
    "title": "Scalar and vector fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere the range of \\(z\\) is infinite. Wind tunnels aren’t really infinitely long, but this is a common approximation if one dimension is much larger than the others. We are neglecting the confinement in \\(z\\).↩︎\nAs a mathematician I don’t worry about units too much. Sometimes units are important, but for this module, you can safely ignore them.↩︎",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Scalar and vector fields"
    ]
  },
  {
    "objectID": "curves.html",
    "href": "curves.html",
    "title": "Curves",
    "section": "",
    "text": "For the purpose of this module, a curve is a one-dimensional object in three-dimensional space. Imagine a hula hoop or a rope. In fact, we’ll generally assume:\nCurves have many important applications in maths and physics, for example, the path of an electron in a magnetic field, or the boundary of a circular disc.",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Curves"
    ]
  },
  {
    "objectID": "curves.html#longer-example",
    "href": "curves.html#longer-example",
    "title": "Curves",
    "section": "Longer example",
    "text": "Longer example\nTo conclude this part of the notes, let’s see a more complicated example.\nFind the total arc-length of the curve which is parameterised as \\[\n{\\mathbf{r}}(t) = \\left[ \\cos t, \\sin t, t \\right], \\quad 0\\leq t\\leq 4\\pi.\n\\]\nThink: What is this shape?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn \\(x\\) and \\(y\\), it just traces out a circle twice. But as it does so, the \\(z\\) co-ordinate increases. So this is a helix, like a slinky.\n\n\n\n\nStep 1: Calculate the tangent vector\n\\[\n{\\mathbf{T}}(t) = \\frac{d{\\mathbf{r}}}{dt} = \\left[-\\sin t, \\cos t, 1\\right].\n\\]\n\n\nStep 2: Find its magnitude\n\\[\n\\left|{\\mathbf{T}}\\right| = \\sqrt{(-\\sin t)^2+(\\cos t)^2 + 1^2} = \\sqrt{2}.\n\\]\n\n\nStep 3: Integrate this\nHere we need to remember that the limits of the integral are the limits of the parameterisation \\[\n\\int_0^{4\\pi} \\sqrt{2} dt = 4\\pi \\sqrt{2}.\n\\]",
    "crumbs": [
      "Chapter 2: Curves, surfaces, volumes (Week 16)",
      "Curves"
    ]
  },
  {
    "objectID": "coordinates.html",
    "href": "coordinates.html",
    "title": "Co-ordinate systems",
    "section": "",
    "text": "One of the key aims of this module is to learn about curvilinear co-ordinates. These are co-ordinate systems where, unlike the usual Cartesian co-ordinates, the basis vectors change direction at different points in space.\nThat may sound confusing, but you should all already have seen one type of curvilinear co-ordinates, (plane) polar co-ordinates. In this module we’ll see two different ways to generalise 2D polar co-ordinates to 3D space, but first we’ll talk through some basic ideas about co-ordinate systems using plane polars as an example.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "coordinates.html#defining-co-ordinate-systems",
    "href": "coordinates.html#defining-co-ordinate-systems",
    "title": "Co-ordinate systems",
    "section": "Defining co-ordinate systems",
    "text": "Defining co-ordinate systems\nTypically we define co-ordinate systems by expressing the Cartesian co-ordinates as a function of some other variables.1\nFor example, in plane polars, \\[\nx = \\rho \\cos \\phi,\\qquad y=\\rho\\sin\\phi.\n\\] The co-ordinates for this system, \\((\\rho,\\phi)\\)2 are, respectively, the distance from the origin and the angle anti-clockwise from the \\(x\\) axis.\nWe also need to know the range of values these co-ordinates can take, let’s use \\(0\\leq \\phi &lt; 2\\pi\\) and \\(\\rho\\geq 0\\).\nOur general position vector in this system is then given by \\[\n{\\mathbf{r}}(\\rho, \\phi) = [\\rho \\cos \\phi,\\,\\rho\\sin\\phi].\n\\] But remember that the square brackets are hiding Cartesian co-ordinates, so this isn’t quite complete. First we need basis vectors.\nAlso notice that the co-ordinates break down at the origin, if \\(\\rho=0\\) then the value of \\(\\phi\\) doesn’t matter. That’s quite common with these systems.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "coordinates.html#basis-vectors-scale-factors",
    "href": "coordinates.html#basis-vectors-scale-factors",
    "title": "Co-ordinate systems",
    "section": "Basis vectors & scale factors",
    "text": "Basis vectors & scale factors\nThe definition of a basis vector is the direction you move from a point in space if you increase a particular co-ordinate. So moving in the direction of \\({\\mathbf{e}}_x\\) increases the value of \\(x\\) but keeps \\(y\\) constant. It’s usually assumed to be a unit vector.\nIn practice, the definition is to differentiate \\({\\mathbf{r}}\\) with respect to the variable and then normalise3: \\[\n\\begin{aligned}\n{\\mathbf{e}}_\\rho &= \\frac{\\partial {\\mathbf{r}}}{\\partial \\rho}/\\left|\\frac{\\partial {\\mathbf{r}}}{\\partial \\rho}\\right|\n\\\\ &= \\frac{\\partial}{\\partial \\rho} [\\rho \\cos \\phi,\\,\\rho\\sin\\phi] / \\left|\\frac{\\partial}{\\partial \\rho} [\\rho \\cos \\phi,\\,\\rho\\sin\\phi]\\right|\n\\\\ &= [\\cos\\phi,\\,\\sin\\phi] / \\sqrt{\\cos^2 \\phi + \\sin^2 \\phi}\n\\\\ &= [\\cos\\phi,\\,\\sin\\phi]\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\n{\\mathbf{e}}_\\phi &= \\frac{\\partial {\\mathbf{r}}}{\\partial \\phi}/\\left|\\frac{\\partial {\\mathbf{r}}}{\\partial \\phi}\\right|\n\\\\ &= \\frac{\\partial}{\\partial \\phi} [\\rho \\cos \\phi,\\,\\rho\\sin\\phi] / \\left|\\frac{\\partial}{\\partial \\phi} [\\rho \\cos \\phi,\\,\\rho\\sin\\phi]\\right|\n\\\\ &= [-\\rho\\sin\\phi,\\,\\rho\\cos\\phi] / \\sqrt{\\rho^2\\cos^2 \\phi + \\rho^2\\sin^2 \\phi}\n\\\\ &= [-\\sin\\phi,\\,\\cos\\phi].\n\\end{aligned}\n\\]\nNote that this also works for Cartestian co-ordinates \\[\n\\begin{aligned}\n{\\mathbf{e}}_x &= \\frac{\\partial {\\mathbf{r}}}{\\partial x}/\\left|\\frac{\\partial {\\mathbf{r}}}{\\partial x}\\right|\n\\\\ &= \\frac{\\partial}{\\partial x} [x,\\,y] / \\left|\\frac{\\partial}{\\partial x} [x,\\,y]\\right|\n\\\\ &= [1,0] / \\sqrt{1^2 + 0^2}\n\\\\ &= [1,0].\n\\end{aligned}\n\\]\nWe also define the scale factors \\(h_\\rho\\) and \\(h_\\phi\\) as the magnitude of the derivatives \\[h_\\rho = \\left|\\frac{\\partial {\\mathbf{r}}}{\\partial \\rho}\\right| = 1,\\] \\[h_\\phi = \\left|\\frac{\\partial {\\mathbf{r}}}{\\partial \\phi}\\right| = \\rho.\\]\n\n\n\n\n\n\nNote\n\n\n\nScale factors will be useful later in the module. For now just get used to the formulas.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "coordinates.html#notating-vectors",
    "href": "coordinates.html#notating-vectors",
    "title": "Co-ordinate systems",
    "section": "Notating vectors",
    "text": "Notating vectors\nYou’re used to the idea that co-ordinates and basis vectors are kind of interchangable. In Cartesians, the point \\((x,y)\\) has position vector \\[\n{\\mathbf{r}} = [x,y] = x {\\mathbf{e}}_x + y {\\mathbf{e}}_y.\n\\] We reserve this square bracket notation for Cartesians, because it doesn’t work in curvilinear co-ordinates.\nAt a point with co-ordinates \\((\\rho,\\phi)\\) in plane polars, the position vector is \\[\n{\\mathbf{r}} = [x,y] = [\\rho \\cos \\phi,\\, \\rho\\sin\\phi] = \\rho {\\mathbf{e}}_\\rho.\n\\] Where does \\(\\phi\\) appear in this expression? The answer is that it’s hidden inside the \\({\\mathbf{e}}_\\rho\\). You have to know the co-ordinates to understand this position vector.\n\n\n\n\n\n\nNote\n\n\n\nI think this is the most confusing thing in this module. If you understand this everything else will be easier.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "coordinates.html#orthogonal-co-ordinate-systems",
    "href": "coordinates.html#orthogonal-co-ordinate-systems",
    "title": "Co-ordinate systems",
    "section": "Orthogonal co-ordinate systems",
    "text": "Orthogonal co-ordinate systems\nA system is called orthogonal if at any point in space, the basis vectors are orthogonal.\nThis seems counterintuitive, since we already said that the basis vectors rotate, but it’s easy to verify that plane polars are orthogonal - we just need to check the dot product: \\[\n{\\mathbf{e}}_\\rho \\cdot {\\mathbf{e}}_\\phi = [\\cos \\phi,\\,\\sin\\phi] \\cdot [-\\sin\\phi,\\,\\cos\\phi] = -\\cos\\phi\\sin\\phi + \\sin\\phi\\cos\\phi = 0.\n\\]\nFor 3D co-ordinate systems, you need to check three different dot products to make sure they’re orthogonal.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "coordinates.html#footnotes",
    "href": "coordinates.html#footnotes",
    "title": "Co-ordinate systems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe could do it the other way round but that’s usually much messier.↩︎\nYou might be more used to using \\((r,\\theta)\\) for 2D polars but we’ll save these for spherical co-ordinates.↩︎\nAs well as \\({\\mathbf{e}}_\\rho\\), you might see other notations like \\(\\hat{{\\mathbf{\\rho}}}\\).↩︎\nWe call this the azimuthal angle.↩︎\nWe call this the polar angle.↩︎",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Coordinate systems"
    ]
  },
  {
    "objectID": "divergence.html",
    "href": "divergence.html",
    "title": "The divergence theorem",
    "section": "",
    "text": "The divergence theorem is similar to Stokes’ theorem, but it relates volumes and surfaces rather than surfaces and curves. As the name suggests, it uses the divergence: \\[\n\\int_V \\nabla\\cdot {\\mathbf{F}}({\\mathbf{r}}) dV = \\int_{\\partial V} {\\mathbf{F}}({\\mathbf{r}})\\cdot d{\\mathbf{S}}.\n\\]\nAs you may be able to guess, \\(\\partial V\\) is the boundary of the volume \\(V\\), with the normal facing out. Again, we may need to do the boundary in more than one piece.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Divergence theorem"
    ]
  },
  {
    "objectID": "divergence.html#greens-first-identity",
    "href": "divergence.html#greens-first-identity",
    "title": "The divergence theorem",
    "section": "Green’s first identity",
    "text": "Green’s first identity\nGreen’s first identity states that, for any scalar fields \\(f\\) and \\(g\\), \\[\n\\int_V \\left(g\\Delta f + \\nabla f \\cdot \\nabla g\\right)dV = \\int_{\\partial V} g\\nabla f \\cdot d{\\mathbf{S}}.\n\\]\nTo prove this, we obviously need to use the divergence theorem. It’s not clear how we can use that on the left-hand side, so let’s start with the right-hand side and use the product rule: \\[\n\\begin{aligned}\n\\int_{\\partial V} g\\nabla f \\cdot d{\\mathbf{S}} &= \\int_{V} \\nabla \\cdot \\left(g\\nabla f\\right) dV\n\\\\ &= \\int_{V} \\left(\\nabla g \\cdot \\nabla f + g\\nabla\\cdot\\nabla f \\right) dV\n\\end{aligned}\n\\] which is exactly what we wanted, when we remember that \\(\\Delta f = \\nabla\\cdot\\nabla f\\).",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Divergence theorem"
    ]
  },
  {
    "objectID": "divergence.html#greens-second-identity",
    "href": "divergence.html#greens-second-identity",
    "title": "The divergence theorem",
    "section": "Green’s second identity",
    "text": "Green’s second identity\nGreen’s second identity states that, for any scalar fields \\(f\\) and \\(g\\), \\[\n\\int_V \\left(f\\Delta g - g\\Delta f\\right)dV = \\int_{\\partial V} \\left(f\\nabla g - g\\nabla f\\right) \\cdot d{\\mathbf{S}}.\n\\] You can get this one from subtracting the first identity from itself with \\(f\\) and \\(g\\) swapped.\nThis one is quite handy when you rearrange it: \\[\n\\int_V f\\Delta g\\ dV = \\int_{\\partial V} \\left(f\\nabla g - g\\nabla f\\right) \\cdot d{\\mathbf{S}} + \\int_V g\\Delta f\\ dV.\n\\] This is basically a form of integration by parts for the Laplacian \\(\\Delta\\).",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Divergence theorem"
    ]
  },
  {
    "objectID": "fouriertransforms.html",
    "href": "fouriertransforms.html",
    "title": "Fourier transforms",
    "section": "",
    "text": "Fourier series work great when we have a function which is perfectly periodic, or defined on a finite interval. But often in practice we have functions defined on the whole real line, when we use an approximation that something in infinitely big.\nIn this case, there isn’t an exact wavelength that fits perfectly, and instead we have to consider all possible wavelengths. This leads us to the Fourier transform.",
    "crumbs": [
      "Chapter 7: Fourier Transforms and Delta Functions (Week 23)",
      "Fourier transforms"
    ]
  },
  {
    "objectID": "fouriertransforms.html#definition-of-the-fourier-transform",
    "href": "fouriertransforms.html#definition-of-the-fourier-transform",
    "title": "Fourier transforms",
    "section": "Definition of the Fourier transform",
    "text": "Definition of the Fourier transform\nThe Fourier transform of a function \\(f(x)\\) is defined as1 \\[\nF(k) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty f(x) e^{-ikx}\\,dx,\n\\] where \\(k\\) is called the wave number.\nThe inverse Fourier transform is given by \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty F(k) e^{ikx}\\,dk.\n\\] These two equations are very similar in form to the Fourier series equations, but now we have integrals over all possible wave numbers \\(k\\) instead of sums over discrete wavelengths.",
    "crumbs": [
      "Chapter 7: Fourier Transforms and Delta Functions (Week 23)",
      "Fourier transforms"
    ]
  },
  {
    "objectID": "fouriertransforms.html#example",
    "href": "fouriertransforms.html#example",
    "title": "Fourier transforms",
    "section": "Example",
    "text": "Example\nFind the Fourier transform of \\[\nf(x) = e^{-\\frac{x^2}{2}}.\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\[\nF(k) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{x^2}{2}} e^{-ikx}\\,dx.\n\\] Combining the exponents gives \\[\nF(k) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{x^2}{2} - ikx}\\,dx = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{x^2 + 2ikx}{2}}\\,dx.\n\\] Completing the square in the exponent: \\[\nx^2 + 2ikx = (x + ik)^2 - k^2,\n\\] so \\[\nF(k) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{(x + ik)^2 - k^2}{2}}\\,dx = \\frac{e^{\\frac{k^2}{2}}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{(x + ik)^2}{2}}\\,dx.\n\\] Now we can make the substitution \\(u = x + ik\\), so \\(du = dx\\). The limits of integration remain \\(-\\infty\\) to \\(\\infty\\) since shifting by a constant (even a complex one) doesn’t change them. Thus, \\[\nF(k) = \\frac{e^{\\frac{k^2}{2}}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac{u^2}{2}}\\,du.\n\\] The integral is just the standard Gaussian integral, which evaluates to \\(\\sqrt{2\\pi}\\). Therefore, \\[\nF(k) = \\frac{e^{\\frac{k^2}{2}}}{\\sqrt{2\\pi}} \\cdot \\sqrt{2\\pi} = e^{\\frac{k^2}{2}}.\n\\] So the Fourier transform of \\(f(x) = e^{-\\frac{x^2}{2\n}}\\) is \\[\nF(k) = e^{\\frac{k^2}{2}}.\n\\]",
    "crumbs": [
      "Chapter 7: Fourier Transforms and Delta Functions (Week 23)",
      "Fourier transforms"
    ]
  },
  {
    "objectID": "fouriertransforms.html#footnotes",
    "href": "fouriertransforms.html#footnotes",
    "title": "Fourier transforms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLots of different conventions exist for the Fourier transform, this is one of the most common.↩︎",
    "crumbs": [
      "Chapter 7: Fourier Transforms and Delta Functions (Week 23)",
      "Fourier transforms"
    ]
  },
  {
    "objectID": "lineintegrals.html",
    "href": "lineintegrals.html",
    "title": "Line integrals",
    "section": "",
    "text": "So far we’ve discussed vector and scalar fields and what it means to differentiate them. In this part of the module we’ll discuss different ways we can integrate these fields over different structures in 3D space, starting with integration over a curve.\nThere’s some general notation here to consider before we dive in. All integrals will be written as \\[\n\\int_A F({\\mathbf{r}}) dB.\n\\] \\(F\\) is the function we’re integrating, \\(A\\) is the set we’re integrating it over (which could be a curve, surface, or volume) and \\(dB\\) is the integral element appropriate for the set (and there are different choices). Instead of \\(\\int\\) you might also see symbols like \\(\\iint\\) or \\(\\oint\\) but these can always be replaced with \\(\\int\\), they just give you a little more context about what you’re integrating.\nIn every case, the steps are going to be the same:",
    "crumbs": [
      "Chapter 3: Integrals (Weeks 17/18)",
      "Line integrals"
    ]
  },
  {
    "objectID": "lineintegrals.html#footnotes",
    "href": "lineintegrals.html#footnotes",
    "title": "Line integrals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes this is written \\(d{\\mathbf{r}}\\)↩︎",
    "crumbs": [
      "Chapter 3: Integrals (Weeks 17/18)",
      "Line integrals"
    ]
  },
  {
    "objectID": "odes.html",
    "href": "odes.html",
    "title": "Examples of ODEs",
    "section": "",
    "text": "ODEs are ordinary differential equations. These are equations which relate ordinary derivatives (not partial derivatives) of a function with each other. You’ve seen these many times before. Because they are ordinary derivatives, there is only one independent variable, which we typically call either \\(x\\) (when talking about space) or \\(t\\) (when talking about time). However, we can have more than one dependent variable.\nHere are some famous examples: \\[\n{\\frac{\\mathrm{d}^2 x}{\\mathrm{d} t^2}} = -\\frac{k}{m}x \\quad\\text{(Simple harmonic motion)}\n\\] This describes, for example, a mass on a spring, or a pendulum undergoing small oscillations. The solutions are sinusoidal functions.\n\\[\n{\\frac{\\mathrm{d} T}{\\mathrm{d} t}} = -k(T - T_{\\text{amb}}) \\quad\\text{(Newton's law of cooling)}\n\\] This describes how an object cools down (or heats up) to the ambient temperature. The solution is an exponential decay (or growth) towards the ambient temperature.\n\\[\n{\\frac{\\mathrm{d} N}{\\mathrm{d} t}} = rN\\left(1-\\frac{N}{K}\\right) \\quad\\text{(Logistic equation for population growth)}\n\\] This describes how a population grows rapidly at first, then slows down as it approaches a maximum sustainable population \\(K\\). The solution is an S-shaped curve called a logistic function.\nThe following is a ‘system’ of ODEs, meaning there are several dependent variables which depend on the same independent variable. Here, \\(x\\), \\(y\\) and \\(z\\) all depend on the time \\(t\\): \\[\n\\begin{aligned}\n{\\frac{\\mathrm{d} x}{\\mathrm{d} t}} &= \\sigma(y - x) \\\\\n{\\frac{\\mathrm{d} y}{\\mathrm{d} t}} &= x(\\rho - z) - y \\\\\n{\\frac{\\mathrm{d} z}{\\mathrm{d} t}} &= xy - \\beta z \\quad\\text{(Lorenz equations for convection)}\n\\end{aligned}\n\\] This system is famous for exhibiting chaotic behaviour for certain values of the parameters \\(\\sigma\\), \\(\\rho\\) and \\(\\beta\\). It was originally derived as a simplified model of atmospheric convection.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Examples of ODEs"
    ]
  },
  {
    "objectID": "odes.html#linear-and-non-linear-odes",
    "href": "odes.html#linear-and-non-linear-odes",
    "title": "Examples of ODEs",
    "section": "Linear and non-linear ODEs",
    "text": "Linear and non-linear ODEs\nAn ODE is called linear if the dependent variable(s) and all their derivatives appear to the first power only, and are not multiplied together. Of those above, the first two are linear, and the last two are non-linear.\nIn many cases, you can solve linear ODEs exactly, using methods you may have seen before. Non-linear ODEs are much harder to solve, and in many cases you can’t find an exact solution at all. Instead, you have to use numerical methods to find approximate solutions.\nIt is important to be able to distinguish the two types, especially as we progress to PDEs.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Examples of ODEs"
    ]
  },
  {
    "objectID": "odes.html#constant-coefficient-odes",
    "href": "odes.html#constant-coefficient-odes",
    "title": "Examples of ODEs",
    "section": "Constant coefficient ODEs",
    "text": "Constant coefficient ODEs\nA linear ODE is said to have constant coefficients if the coefficients (the numbers multiplying the dependent variable and its derivatives) are constants, not functions of the independent variable. For example, in the simple harmonic motion equation above, \\(k\\) and \\(m\\) are constants, so it has constant coefficients.\nA general first order linear, homogeneous ODE with constant coefficients has the form \\[\na{\\frac{\\mathrm{d} y}{\\mathrm{d} x}} + by = 0,\n\\] and this has the solution \\[\ny(x) = C e^{-\\frac{b}{a}x},\n\\] where \\(C\\) is an arbitrary constant.\nA general second order linear, homogeneous ODE with constant coefficients has the form \\[\na{\\frac{\\mathrm{d}^2 y}{\\mathrm{d} x^2}} + b{\\frac{\\mathrm{d} y}{\\mathrm{d} x}} + cy = 0,\n\\] and this has the solution \\[\ny(x) = C_1 e^{r_1 x} + C_2 e^{r_2 x},\n\\] where \\(C_1\\) and \\(C_2\\) are arbitrary constants, and \\(r_1\\) and \\(r_2\\) are the roots of the characteristic equation \\[\nar^2 + br + c = 0.\n\\] which may be real or complex.\nNotice the exponentials in both cases. If in doubt, try an exponential function as a solution!\n\n\n\n\n\n\nNote\n\n\n\nThis stuff shouldn’t be new to you, but we’ll see it a lot when we’re solving more complicated equations, so it’s worth making sure you understand it.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Examples of ODEs"
    ]
  },
  {
    "objectID": "odes.html#existence-and-uniqueness-of-solutions",
    "href": "odes.html#existence-and-uniqueness-of-solutions",
    "title": "Examples of ODEs",
    "section": "Existence and uniqueness of solutions",
    "text": "Existence and uniqueness of solutions\nWhen we turn to PDEs, it becomes very important to know whether a given equation has a solution, and whether that solution is unique. For ODEs, and especially for those second order, constant coefficient ODEs, there are well-established theorems that give conditions for existence and uniqueness of solutions.\nIt is important, however, to think about what boundary or initial conditions you need to specify in order to get a unique solution. For example, for a second order ODE, you typically need to specify two conditions, such specifying both the value of the function and its first derivative at a particular point, but this may not always be the case.\nFor example, consider the equation \\[\n{\\frac{\\mathrm{d}^2 y}{\\mathrm{d} x^2}} + {\\frac{\\mathrm{d} y}{\\mathrm{d} x}} = 0.\n\\] This is a second order ODE, so we need two conditions to specify a unique solution.\nThe general solution to this ODE is \\[\ny(x) = C_1 + C_2 e^{-x},\n\\] where \\(C_1\\) and \\(C_2\\) are arbitrary constants.\nIf we specify \\[\ny(0) = 1, \\quad {\\frac{\\mathrm{d} y}{\\mathrm{d} x}}(0) = -1,\n\\] then we find the unique solution \\[\ny(x) = 1 - e^{-x}.\n\\]\nHowever, if we instead specify that \\[\n{\\frac{\\mathrm{d} y}{\\mathrm{d} x}}(0) = -1, \\quad {\\frac{\\mathrm{d}^2 y}{\\mathrm{d} x^2}}(0) = 1,\n\\] then we can have any solution of the form \\[\ny(x) = C_1 + e^{-x},\n\\] where \\(C_1\\) can be any constant. Thus, the solution is not unique in this case.\nOn the other hand, if we specify \\[\n{\\frac{\\mathrm{d} y}{\\mathrm{d} x}}(0) = -1, \\quad {\\frac{\\mathrm{d}^2 y}{\\mathrm{d} x^2}}(0) = -1,\n\\] then there is no solution that satisfies both conditions, so in this case a solution does not exist.\n\n\n\n\n\n\nNote\n\n\n\nI won’t ever ask you to solve an equation that has no solution, though I might ask you to identify when this is the case.\n\n\n\nExample\nFor the linear ODE \\[\n{\\frac{\\mathrm{d}^2 y}{\\mathrm{d} x^2}} = -y,\n\\] subject to the boundary conditions \\(y(0) = 0\\) and \\(y'(0) = 1\\) for some constant \\(L\\), find a solution and show that it is unique.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can guess a solution to this ODE \\[\ny(x) = sin(x),\n\\] which satisfies the ODE and the boundary conditions.\nTo show that this solution is unique, suppose there is another solution \\(y_2(x)\\) that also satisfies the ODE and the boundary conditions. Then the difference \\(v(x) = y(x) - y_2(x)\\) satisfies the ODE \\[\n{\\frac{\\mathrm{d}^2 v}{\\mathrm{d} x^2}} = -v,\n\\] with the boundary conditions \\(v(0) = y(0) - y_2(0) = 0\\) and \\(v'(0) = y'(0) - y_2'(0) = 0\\).\nThe general solution to this ODE is \\[\nv(x) = C_1 \\sin(x) + C_2 \\cos(x),\n\\] where \\(C_1\\) and \\(C_2\\) are arbitrary constants. Applying the boundary conditions gives \\(C_1 = 0\\) and \\(C_2 = 0\\), so the only solution is the trivial solution \\(v(x) = 0\\). Therefore, we have shown that \\(y(x) = y_2(x)\\), and hence the solution is unique.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Weeks 20/21)",
      "Examples of ODEs"
    ]
  },
  {
    "objectID": "potentials.html",
    "href": "potentials.html",
    "title": "Vector and scalar potentials",
    "section": "",
    "text": "The final part of this chapter concerns special types of vector fields.\nWe call a field \\({\\mathbf{A}}\\) solenoidal if \\[\n\\nabla\\cdot {\\mathbf{A}} = 0\n\\] and we call a field \\({\\mathbf{B}}\\) irrotational if \\[\n\\nabla\\times{\\mathbf{B}} = {\\mathbf{0}}.\n\\]",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Potentials"
    ]
  },
  {
    "objectID": "potentials.html#irrotational-fields",
    "href": "potentials.html#irrotational-fields",
    "title": "Vector and scalar potentials",
    "section": "Irrotational fields",
    "text": "Irrotational fields\nThese are sometimes called curl free fields, for obvious reasons. The key theorem is:\n\n\n\n\n\n\nImportant\n\n\n\n\\[\\nabla\\times{\\mathbf{A}} = {\\mathbf{0}} \\text{ if and only if }{\\mathbf{A}} = \\nabla \\phi\\text{ for some scalar field }\\phi.\\]\n\n\nWe call \\(\\phi\\) a scalar potential for \\({\\mathbf{A}}\\).\nProving \\(\\impliedby\\) is easy: If \\({\\mathbf{A}} = \\nabla \\phi\\) then \\[\n\\nabla\\times {\\mathbf{A}} = \\nabla \\times \\nabla \\phi = 0,\n\\] this was one of our second derivative identities.\nThe proof in the other direction is much more complicated, but for any given example you can make it work.\n\nExample\nFind a scalar potential for \\[\n{\\mathbf{A}} = [x^2,y^2,0].\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst check that \\(\\nabla\\times{\\mathbf{A}}={\\mathbf{0}}\\), otherwise it doesn’t work.\nThen we need to find \\(\\phi\\) such that \\(\\nabla \\phi = [x^2,y^2,0]\\).\nMaybe you can spot it directly, but otherwise we can break it down. We need to find \\(\\phi(x,y,z)\\) such that \\[\n\\begin{aligned}\n{\\frac{\\partial \\phi}{\\partial x}} &= x^2,\\qquad\\text{(1)} \\\\\n{\\frac{\\partial \\phi}{\\partial y}} &= y^2,\\qquad\\text{(2)}\\\\\n{\\frac{\\partial \\phi}{\\partial z}} &= 0.\\qquad\\text{(3)}\n\\end{aligned}\n\\]\nIntegrating the (1), we have \\[\\phi(x,y,z) = \\frac{1}{3} x^3 + C_1(y,z)\\] Here \\(C_1\\) is just a normal constant of integration, but because we integrated a partial derivative, this constant can depend on \\(y\\) and \\(z\\).\nThen (2) becomes \\[{\\frac{\\partial C_1}{\\partial y}} = y^2\\] so \\[\nC_1 = \\frac{1}{3} y^3 + C_2(z).\\] \\(C_1\\) depends only on \\(y\\) and \\(z\\), so the new constant of integration \\(C_2\\) depends only on \\(z\\). So we have \\[\n\\phi = \\frac{1}{3}x^3 + \\frac{1}{3}y^3 + C_2(z),\n\\] so (3) gives \\[\n\\frac{dC_2}{dz} = 0\n\\] so \\(C_2\\) is just a constant.\nSo finally we have \\[\n\\phi(x,y,z) = \\frac{1}{3}x^3 + \\frac{1}{3}y^3 + C_2,\n\\] where \\(C_2\\) is any constant.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nScalar potentials are not unique - you can add any constant to \\(\\phi\\) and it’s still valid.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Potentials"
    ]
  },
  {
    "objectID": "potentials.html#solenoidal-fields",
    "href": "potentials.html#solenoidal-fields",
    "title": "Vector and scalar potentials",
    "section": "Solenoidal fields",
    "text": "Solenoidal fields\nThese are also called incompressible or divergence free1. The key theorem here is:\n\n\n\n\n\n\nImportant\n\n\n\n\\[\\nabla\\cdot{\\mathbf{A}} = {\\mathbf{0}} \\text{ if and only if }{\\mathbf{A}} = \\nabla\\times {\\mathbf{B}}\\text{ for some vector field }{\\mathbf{B}}.\\]\n\n\nWe call \\({\\mathbf{B}}\\) a vector potential for \\({\\mathbf{A}}\\). Again, you can easily prove that \\(\\nabla\\times{\\mathbf{B}}\\) is solenoidal, but the other way is harder. You can also easily prove this:\n\n\n\n\n\n\nImportant\n\n\n\nVector potentials are not unique - If \\({\\mathbf{B}}\\) is a vector potential for \\({\\mathbf{A}}\\) then so is \\({\\mathbf{B}}+\\nabla\\phi\\) for any scalar field \\(\\phi\\).2\n\n\nThis non-uniqueness is very important, because it means we can choose properties we want \\({\\mathbf{B}}\\) to have. One convenient choice is often \\[B_3=0,\\] no component in the \\(z\\) direction.\n\nExample\nFind a vector potential for \\[\n{\\mathbf{A}} = [x+y^2, -y+z^2, y^2]\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis works the same as for scalar potentials, but it gets much messier. I strongly recommend staring at it for 30 seconds to see if you can guess a potential that works, before trying as follows:\nFirst, you could check it’s actually solenoidal. That’s quite quick in this case.\nWe need to find \\({\\mathbf{B}}=[B_1,B_2,B_3]\\) such that \\[\n[x+y^2, -y+z^2, y^2] = \\nabla\\times{\\mathbf{B}} = [{\\frac{\\partial B_3}{\\partial y}}-{\\frac{\\partial B_2}{\\partial z}},\\,{\\frac{\\partial B_1}{\\partial z}}-{\\frac{\\partial B_3}{\\partial x}},\\,{\\frac{\\partial B_2}{\\partial x}}-{\\frac{\\partial B_1}{\\partial y}}].\n\\]\nLet’s make the gauge choice that \\(B_3=0\\), so then we get \\[x+y^2 = -{\\frac{\\partial B_2}{\\partial z}},\\] \\[-y+z^2 = {\\frac{\\partial B_1}{\\partial z}},\\] \\[y^2 = {\\frac{\\partial B_2}{\\partial x}} - {\\frac{\\partial B_1}{\\partial y}}.\\]\nIntegrating the first two of these, we have: \\[\nB_2 = -x z - y^2 z + f(x, y)\n\\] and \\[\nB_1 = -y z + \\frac{1}{3} z^3 + g(x, y)\n\\] where \\(f\\) and \\(g\\) are some unknown integration constant functions.\nNow, substitute \\(B_1\\) and \\(B_2\\) into the third equation: \\[\ny^2 = (-z + {\\frac{\\partial f}{\\partial x}}) - (-z + {\\frac{\\partial g}{\\partial y}}) = {\\frac{\\partial f}{\\partial x}} - {\\frac{\\partial g}{\\partial y}}\n\\]\nWe still have some freedom here. A simple choice is \\(f(x, y) = 0\\) and \\(g(x, y) = -\\frac{1}{3} y^3\\): \\[\n{\\frac{\\partial f}{\\partial x}} = 0,\\quad {\\frac{\\partial g}{\\partial y}} = -y^2\n\\] So, \\[\ny^2 = 0 - (-y^2) = y^2\n\\] This works.\nTherefore, a vector potential is: \\[\n{\\mathbf{B}} = \\left[\n    -y z + \\frac{1}{3} z^3 - \\frac{1}{3} y^3,\\;\n    -x z - y^2 z,\\;\n    0.\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs with many things in maths, finding the potentials is slow but checking they are valid is quick. Always check!",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Potentials"
    ]
  },
  {
    "objectID": "potentials.html#footnotes",
    "href": "potentials.html#footnotes",
    "title": "Vector and scalar potentials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn physics, the magnetic field is a solenoidal field. This is where the name comes from.↩︎\nThis is called gauge freedom in physics.↩︎",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Potentials"
    ]
  },
  {
    "objectID": "secondderivs.html",
    "href": "secondderivs.html",
    "title": "Derivatives of products and second derivatives",
    "section": "",
    "text": "This chapter focuses on certain identities we can derive by combining the different 3D derivatives. You should get used to using these identities, but you also need to be able to prove them from the definitions.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Products & second derivatives"
    ]
  },
  {
    "objectID": "secondderivs.html#derivatives-of-products",
    "href": "secondderivs.html#derivatives-of-products",
    "title": "Derivatives of products and second derivatives",
    "section": "Derivatives of products",
    "text": "Derivatives of products\nYou’re familiar with the product rule for ordinary derivatives: \\[\n\\frac{d}{dx}(fg) = \\frac{df}{dx}g + f\\frac{dg}{dx}.\n\\] How does this translate to our new grad, div and curl? The first thing to consider is what kinds of products we have.\nThink: What possible products exist between any two of scalar fields \\(\\phi\\) and \\(\\psi\\) and vector fields \\({\\mathbf{u}}\\) and \\({\\mathbf{v}}\\), and is the result a scalar or vector field?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\phi\\psi\\) is a scalar field\n\\(\\phi{\\mathbf{u}}\\) is a vector field\n\\({\\mathbf{u}}\\cdot{\\mathbf{v}}\\) is a scalar field\n\\({\\mathbf{u}}\\times{\\mathbf{v}}\\) is a vector field\n\n\n\nFor each product which is a scalar we can work out a formula for the gradient1, and for each vector we can work out formulas for the divergence and curl. Here is the list:\n\nGradient of a product of scalars: \\[\n\\nabla(\\phi\\psi) = (\\nabla\\phi)\\psi + \\phi(\\nabla\\psi)\n\\]\nGradient of a dot product: \\[\n\\nabla({\\mathbf{u}}\\cdot{\\mathbf{v}}) = ({\\mathbf{u}}\\cdot\\nabla){\\mathbf{v}} + ({\\mathbf{v}}\\cdot\\nabla){\\mathbf{u}} + {\\mathbf{u}}\\times(\\nabla\\times{\\mathbf{v}}) + {\\mathbf{v}}\\times(\\nabla\\times{\\mathbf{u}})\n\\]\nDivergence of a scalar times a vector: \\[\n\\nabla\\cdot(\\phi{\\mathbf{u}}) = {\\mathbf{u}}\\cdot\\nabla\\phi + \\phi\\nabla\\cdot{\\mathbf{u}}\n\\]\nDivergence of a cross product: \\[\n\\nabla\\cdot({\\mathbf{u}}\\times{\\mathbf{v}}) = {\\mathbf{v}}\\cdot(\\nabla\\times{\\mathbf{u}}) - {\\mathbf{u}}\\cdot(\\nabla\\times{\\mathbf{v}})\n\\]\nCurl of a scalar times a vector: \\[\n\\nabla\\times(\\phi{\\mathbf{u}}) = (\\nabla\\phi)\\times{\\mathbf{u}} + \\phi(\\nabla\\times{\\mathbf{u}})\n\\]\nCurl of a cross product: \\[\n\\nabla\\times({\\mathbf{u}}\\times{\\mathbf{v}}) = {\\mathbf{u}}(\\nabla\\cdot{\\mathbf{v}}) - {\\mathbf{v}}(\\nabla\\cdot{\\mathbf{u}}) + ({\\mathbf{v}}\\cdot\\nabla){\\mathbf{u}} - ({\\mathbf{u}}\\cdot\\nabla){\\mathbf{v}}\n\\]\n\n\nExample\nFor a general scalar field \\(\\phi\\) and vector field \\({\\mathbf{u}}\\), show that \\[\n\\nabla \\cdot \\left(\\phi{\\mathbf{u}}\\right) = {\\mathbf{u}}\\cdot \\nabla \\phi + \\phi \\nabla\\cdot{\\mathbf{u}}.\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(\\phi\\) be a scalar field and \\({\\mathbf{u}} = [u_1, u_2, u_3]\\) a vector field. Then, \\[\n\\begin{aligned}\n\\nabla \\cdot (\\phi {\\mathbf{u}}) &= {\\frac{\\partial }{\\partial x}}(\\phi u_1) + {\\frac{\\partial }{\\partial y}}(\\phi u_2) + {\\frac{\\partial }{\\partial z}}(\\phi u_3)\\\\\n&= u_1 {\\frac{\\partial \\phi}{\\partial x}} + \\phi {\\frac{\\partial u_1}{\\partial x}} + u_2 {\\frac{\\partial \\phi}{\\partial y}} + \\phi {\\frac{\\partial u_2}{\\partial y}} + u_3 {\\frac{\\partial \\phi}{\\partial z}} + \\phi {\\frac{\\partial u_3}{\\partial z}}\\\\\n&= u_1 {\\frac{\\partial \\phi}{\\partial x}} + u_2 {\\frac{\\partial \\phi}{\\partial y}} + u_3 {\\frac{\\partial \\phi}{\\partial z}} + \\phi \\left({\\frac{\\partial u_1}{\\partial x}} + {\\frac{\\partial u_2}{\\partial y}} + {\\frac{\\partial u_3}{\\partial z}}\\right)\\\\\n&= {\\mathbf{u}} \\cdot \\nabla \\phi + \\phi (\\nabla \\cdot {\\mathbf{u}}).\n\\end{aligned}\n\\]",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Products & second derivatives"
    ]
  },
  {
    "objectID": "secondderivs.html#second-derivatives",
    "href": "secondderivs.html#second-derivatives",
    "title": "Derivatives of products and second derivatives",
    "section": "Second derivatives",
    "text": "Second derivatives\nFor normal derivatives, you know \\(\\frac{d}{dx} \\frac{dy}{dx} = \\frac{d^2y}{dx^2}\\) - this is basically just a definition.\nIn 3D, it’s more complicated. Of course, we can only take the gradient of a scalar field, and only take the divergence or curl of a vector field so the possibilities are, for a vector field \\({\\mathbf{u}}\\) and a scalar field \\(\\phi\\):\n\n\\(\\nabla\\cdot(\\nabla\\phi)\\) gives a scalar field\n\\(\\nabla\\times(\\nabla\\phi)\\) gives a vector field\n\\(\\nabla (\\nabla \\cdot {\\mathbf{u}})\\) gives a vector field\n\\(\\nabla \\times(\\nabla\\times {\\mathbf{u}})\\) gives a vector field\n\\(\\nabla \\cdot(\\nabla \\times{\\mathbf{u}})\\) gives a scalar field\n\nAny other combination, like \\(\\nabla\\cdot (\\nabla \\cdot {\\mathbf{u}})\\) is not defined.\nActually, there are two extra rules which you need to learn, and you can prove: \\[\\nabla\\cdot(\\nabla \\times {\\mathbf{u}}) = 0\\] \\[\\nabla\\times(\\nabla \\phi) = {\\mathbf{0}}\\]\n\nExample\nFor any scalar field \\(\\phi\\), show that \\(\\nabla\\times(\\nabla \\phi) = {\\mathbf{0}}\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\nabla\\times(\\nabla \\phi) &= \\nabla\\times\\left[{\\frac{\\partial \\phi}{\\partial x}},\\,{\\frac{\\partial \\phi}{\\partial y}},\\,{\\frac{\\partial \\phi}{\\partial z}},\\,\\right] \\\\\n&= \\left[ {\\frac{\\partial }{\\partial y}}{\\frac{\\partial \\phi}{\\partial z}} - {\\frac{\\partial }{\\partial z}}{\\frac{\\partial \\phi}{\\partial y}},\\,{\\frac{\\partial }{\\partial z}}{\\frac{\\partial \\phi}{\\partial x}} - {\\frac{\\partial }{\\partial x}}{\\frac{\\partial \\phi}{\\partial z}},\\,{\\frac{\\partial }{\\partial x}}{\\frac{\\partial \\phi}{\\partial y}} - {\\frac{\\partial }{\\partial y}}{\\frac{\\partial \\phi}{\\partial x}}\\right] \\\\\n&= \\left[ {\\frac{\\partial }{\\partial y}}{\\frac{\\partial \\phi}{\\partial z}} - {\\frac{\\partial }{\\partial y}}{\\frac{\\partial \\phi}{\\partial z}},\\,{\\frac{\\partial }{\\partial z}}{\\frac{\\partial \\phi}{\\partial x}} - {\\frac{\\partial }{\\partial z}}{\\frac{\\partial \\phi}{\\partial x}},\\,{\\frac{\\partial }{\\partial x}}{\\frac{\\partial \\phi}{\\partial y}} - {\\frac{\\partial }{\\partial x}}{\\frac{\\partial \\phi}{\\partial y}}\\right]  \\\\\n&= \\left[0,0,0\\right]\\\\\n&= {\\mathbf{0}}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe key with all of these proofs is the symmetry of a mixed partial derivatives, i.e. \\(\\frac{\\partial^2 f}{\\partial x\\partial y} = \\frac{\\partial^2 f}{\\partial y\\partial x}\\)2.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Products & second derivatives"
    ]
  },
  {
    "objectID": "secondderivs.html#the-laplacian",
    "href": "secondderivs.html#the-laplacian",
    "title": "Derivatives of products and second derivatives",
    "section": "The Laplacian",
    "text": "The Laplacian\nThe remaining three second derivatives, \\(\\nabla\\cdot(\\nabla\\phi)\\), \\(\\nabla \\times(\\nabla\\times {\\mathbf{u}})\\) and \\(\\nabla (\\nabla \\cdot {\\mathbf{u}})\\) are related to another operator called the Laplacian, which is given the symbol \\(\\Delta\\) or \\(\\nabla^2\\). We’ll use \\(\\Delta\\) in this module.3\nFor a scalar field \\(\\phi\\), \\[\n\\begin{aligned}\n\\nabla\\cdot(\\nabla\\phi) &= \\nabla\\cdot\\left[{\\frac{\\partial \\phi}{\\partial x}},{\\frac{\\partial \\phi}{\\partial y}},{\\frac{\\partial \\phi}{\\partial z}}\\right]\n\\\\&= {\\frac{\\partial }{\\partial x}}{\\frac{\\partial \\phi}{\\partial x}} + {\\frac{\\partial }{\\partial y}}{\\frac{\\partial \\phi}{\\partial y}} + {\\frac{\\partial }{\\partial z}}{\\frac{\\partial \\phi}{\\partial z}}\n\\\\&= \\frac{\\partial^2 \\phi}{\\partial x^2} + \\frac{\\partial^2 \\phi}{\\partial y^2}+ \\frac{\\partial^2 \\phi}{\\partial z^2}\n\\end{aligned}\n\\] This is what we define to be the \\(\\Delta \\phi\\), the (scalar) Laplacian of \\(\\phi\\).\nFor a vector field \\({\\mathbf{u}}\\),4 \\[\n\\begin{aligned}\n\\nabla(\\nabla\\cdot {\\mathbf{u}}) - \\nabla\\times(\\nabla\\times{\\mathbf{u}}) &= \\left[\\frac{\\partial^2 u_1}{\\partial x^2} + \\frac{\\partial^2 u_1}{\\partial y^2}+ \\frac{\\partial^2 u_1}{\\partial z^2},\\,\\frac{\\partial^2 u_2}{\\partial x^2} + \\frac{\\partial^2 u_2}{\\partial y^2}+ \\frac{\\partial^2 u_2}{\\partial z^2},\\,\\frac{\\partial^2 u_3}{\\partial x^2} + \\frac{\\partial^2 u_3}{\\partial y^2}+ \\frac{\\partial^2 u_3}{\\partial z^2}\\right]\\\\\n&= \\left[\\Delta u_1,\\,\\Delta u_2,\\,\\Delta u_3\\right].\n\\end{aligned}\n\\] This is what we define to be \\(\\Delta {\\mathbf{u}}\\), the (vector) Laplacian of \\({\\mathbf{u}}\\).\n\n\n\n\n\n\nImportant\n\n\n\nIn Cartesians, the vector Laplacian is the scalar Laplacian of each of the components. In curvilinear co-ordinates, this isn’t true.",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Products & second derivatives"
    ]
  },
  {
    "objectID": "secondderivs.html#footnotes",
    "href": "secondderivs.html#footnotes",
    "title": "Derivatives of products and second derivatives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can do directional derivatives too but it’s basically the same.↩︎\nWhich holds as long as the second derivatives are continuous, which they always will be in this module.↩︎\nYou won’t get penalised if you prefer to write \\(\\nabla^2\\).↩︎\nTry this as an exercise, it takes a while.↩︎",
    "crumbs": [
      "Chapter 1: Fields, co-ordinates and derivatives (Weeks 14/15)",
      "Products & second derivatives"
    ]
  },
  {
    "objectID": "stokes.html",
    "href": "stokes.html",
    "title": "Stokes’ theorem",
    "section": "",
    "text": "Stokes’ theorem is one form of the Fundamental Theorem of Calculus in three dimensions, and it relates surface integrals to line integrals. Specifically, it says that \\[\n\\int_S (\\nabla \\times {\\mathbf{F}})\\cdot d{\\mathbf{S}} = \\int_{\\partial S} {\\mathbf{F}} \\cdot d{\\mathbf{s}}.\n\\] The only thing that’s new here is the curve \\(\\partial S\\). It says that\nWe won’t attempt a proof of this theorem, though in class we’ll see some intuition behind it. It is possible to prove it with techniques you know, but once you learn differential geometry it’s a special case of a much easier theorem.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "stokes.html#the-boundary-of-a-surface",
    "href": "stokes.html#the-boundary-of-a-surface",
    "title": "Stokes’ theorem",
    "section": "The boundary of a surface",
    "text": "The boundary of a surface\nThe symbol \\(\\partial S\\) is the boundary of the surface \\(S\\).1 The surface is a 2D object, and the boundary is a curve, a 1D object. Specifically, given the direction the normal is pointing, the boundary obeys the right-hand rule as per the diagram.\n\nThink: what’s the boundary of the surface \\(x^2+y^2+z^2=1\\), \\(z\\geq 0\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a hemisphere. Hopefully you can immediately spot the boundary, but if not, try substituting the condition \\(z\\geq 0\\) at its limit, i.e. \\(z=0\\), into the equation \\(x^2+y^2+z^2=0\\).\nSo we get \\(x^2+y^2=0\\), \\(z=0\\), which is a circle.\nI didn’t tell you the direction of the normal, so you can’t tell me which way this is oriented.\n\n\n\nSometimes, the boundary of a surface has multiple parts. In this case, you can split the line integral into several simpler ones, making sure that at each separate curve, the right-hand rule is respected. The example below should make this a bit clearer.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "stokes.html#conditions",
    "href": "stokes.html#conditions",
    "title": "Stokes’ theorem",
    "section": "Conditions",
    "text": "Conditions\nFor the theorem to apply, the surface \\(S\\) has to be oriented. That means that the normal is well-defined everywhere on the surface.\nMost things you can think of are orientable: a sphere, a sheet of paper, a donut. You then have to choose which direction the normal goes and it becomes oriented.\nThe usual counter example is the Mobius band: this doesn’t have a well defined normal, because you can point it one way, and then follow it all the way around the band and it’s suddenly pointing the opposite way.\n\nExample\nLet \\(S\\) be the curved surface of the cylinder \\(x^2+y^2=1\\), \\(0\\leq z\\leq 2\\), with normal pointing out. Evaluate the integral \\[\n\\int_S \\left(\\nabla\\times [z^2,x^2,y^2]\\right) \\cdot d{\\mathbf{S}}\n\\] both directly and using Stokes’ theorem.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, sketch the surface to understand it and its boundary.\n\nDirectly\nThe obvious parameterisation is, taking inspiration from cylindrical co-ordinates, \\[\n[x,y,z] = [\\cos u,\\sin u, v]\\qquad 0\\leq u&lt;2\\pi,\\,0\\leq v \\leq 2.\n\\]\nThen we have \\[\nd{\\mathbf{S}} = {\\frac{\\partial {\\mathbf{r}}}{\\partial u}}\\times{\\frac{\\partial {\\mathbf{r}}}{\\partial v}} du\\, dv = [-\\sin u,\\cos u,0]\\times[0,0,1] du\\,dv = [\\cos u,\\sin u,0] du\\,dv\n\\] and this normal does point out, as we want.\nThe integrand is \\[\\nabla\\times [z^2,x^2,y^2] = [2y, 2z, 2x] = [2\\sin u,2v,2\\cos u]\\] so putting it all together, \\[\n\\begin{aligned}\n\\int_S \\left(\\nabla\\times [z^2,x^2,y^2]\\right) \\cdot d{\\mathbf{S}} &= \\int_0^{2\\pi} du \\int_0^2 dv [2\\sin u,2v,2\\cos u]\\cdot[\\cos u,\\sin u,0]\\\\\n&=\\int_0^{2\\pi} du \\int_0^2 dv \\left(2\\sin u \\cos u + 2v\\sin u\\right)\\\\\n&=\\int_0^{2\\pi} du \\int_0^2 dv \\left(\\sin 2u + 2v\\sin u\\right)\\\\\n&= \\int_0^{2\\pi} du \\left(2\\sin 2u + 4\\sin u\\right)\\\\\n&= 0.\n\\end{aligned}\n\\]\n\n\nStokes theorem\nWe need to evaluate \\[\n\\int_{\\partial S} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}},\n\\] but crucially \\(\\partial S\\) has two parts, that we will call \\(\\gamma_1\\) and \\(\\gamma_2\\), and we write \\(\\partial S = \\gamma_1+\\gamma_2\\).\n\nFor \\(\\gamma_1\\), we parameterise \\[[x,y,z] = [\\cos t,\\,\\sin t,\\,0], \\qquad 0\\leq t &lt; 2\\pi.\\] Then \\(d{\\mathbf{s}} = [-\\sin t,\\cos t,0]dt\\) so the integral is \\[\n\\int_{\\gamma_1} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}} = \\int_0^{2\\pi} [0,\\cos^2 t,\\sin^2 t]\\cdot [-\\sin t,\\cos t,0]dt =\n\\int_0^{2\\pi} \\cos^3 t\\, dt = 0.\n\\]\nFor \\(\\gamma_2\\), we parameterise \\[[x,y,z] = [\\cos t,\\,-\\sin t,\\,2], \\qquad 0\\leq t &lt; 2\\pi.\\] Notice the extra minus sign! This is because we need to go the opposite direction from usual, from the diagram. Now \\(d{\\mathbf{s}} = [\\sin t,\\cos t,0]dt\\) so the integral is \\[\n\\int_{\\gamma_2} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}} = \\int_0^{2\\pi} [4,\\cos^2 t,\\sin^2 t]\\cdot [\\sin t,\\cos t,0]dt =\n\\int_0^{2\\pi} \\left(4\\sin t + \\cos^3 t\\right)\\, dt = 0.\n\\] So overall \\[\n\\int_S \\left(\\nabla\\times [z^2,x^2,y^2]\\right) \\cdot d{\\mathbf{S}}=\\int_{\\partial S} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}} = \\int_{\\gamma_1} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}} + \\int_{\\gamma_2} [z^2,x^2,y^2] \\cdot d{\\mathbf{s}} = 0,\n\\] which is exactly the same answer as we got before!\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo change the direction of a line integral, you can just subtract the integral instead of changing the direction of the parameterisation.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "stokes.html#closed-surface",
    "href": "stokes.html#closed-surface",
    "title": "Stokes’ theorem",
    "section": "Closed surface",
    "text": "Closed surface\nA closed surface is one that has an “inside” and an “outside” - the surface forms a barrier in 3D space. Think of a bubble, or a donut.\nThink: What’s the boundary of a closed surface? How does Stokes’ theorem apply?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt doesn’t have a boundary! So the right-hand side of Stoke’s theorem is zero, and we just have \\[\n\\int_S (\\nabla \\times {\\mathbf{F}})\\cdot d{\\mathbf{S}} = 0\n\\]\n\n\n\nFor a closed surface, we sometimes use the symbol \\(\\oint\\). The theorem is \\[\n\\oint_S (\\nabla \\times {\\mathbf{F}})\\cdot d{\\mathbf{S}} = 0\n\\] and this is most obviously a connection between integration and differentiation: the flux integral and the curl are basically opposites for a closed surface.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "stokes.html#example-1",
    "href": "stokes.html#example-1",
    "title": "Stokes’ theorem",
    "section": "Example",
    "text": "Example\nUsing Stokes’ theorem and the fact that \\[\n\\nabla \\times [xz,\\,xy,\\,yz] = [z,x,y],\n\\] evaluate the integral \\[\n\\oint_S [x+z,\\,y+x,\\,z+y]\\cdot d{\\mathbf{S}}\n\\] where \\(S\\) is the sphere \\(x^2+y^2+z^2=1\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nObviously we could compute this integral directly, but the question wants us to use the theorem. It’s not exactly in the form we need, but notice \\[[x+z,\\,y+x,\\,z+y] = [x,y,z]+[z,x,y] = \\hat{{\\mathbf{n}}} + \\nabla \\times [xz,\\,xy,\\,yz].\\]\nSo then \\[\n\\oint_S [x+z,\\,y+x,\\,z+y]\\cdot d{\\mathbf{S}} = \\oint_S \\hat{{\\mathbf{n}}} \\cdot \\hat{{\\mathbf{n}}} dS + \\oint_S \\left(\\nabla \\times [xz,\\,xy,\\,yz]\\right) \\cdot d{\\mathbf{S}}.\n\\] The second integral is zero by the divergence theorem, because \\(S\\) is a closed surface.\nA unit vector dotted with itself is 1, so the first integral simplies to \\[\\oint_S  dS = 4\\pi,\\] the surface area of \\(S\\), which is the final answer.",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "stokes.html#footnotes",
    "href": "stokes.html#footnotes",
    "title": "Stokes’ theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy do we use the same symbol as for partial derivatives, you may wonder? This will require proper differential geometry to understand, for now it’s just a coincidence.↩︎",
    "crumbs": [
      "Chapter 4: Vector Calculus Theorems (Week 19)",
      "Stokes' theorem"
    ]
  },
  {
    "objectID": "surfaceintegrals.html#special-case-surface-area",
    "href": "surfaceintegrals.html#special-case-surface-area",
    "title": "Surface integrals",
    "section": "Special case: surface area",
    "text": "Special case: surface area\nAs promised, we can now calculate the area of surfaces. This is simply the empty integral \\[\n\\int_S dS = \\int du \\int dv \\left|{\\frac{\\partial {\\mathbf{r}}}{\\partial u}}\\times{\\frac{\\partial {\\mathbf{r}}}{\\partial v}}\\right|\n\\]\n\nExample: surface area of a sphere\nCalculate the surface area of a sphere of radius \\(R\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe’re not told the exact surface, but let’s assume the sphere is centred at the origin. Then we can parameterise by taking some inspiration from spherical polar co-ordinates: \\[[x,y,z] = [R\\cos u\\sin v,\\,R\\sin u\\sin v,\\,R\\cos v], \\qquad 0\\leq u &lt; 2\\pi,\\, 0\\leq v\\leq \\pi.\\] This isn’t exactly polar co-ordinates because \\(R\\) is a constant not a variable, and \\(u\\) and \\(v\\) just parameterise the surface not the whole of 3D space.\nNow we can just plug this into the formula above: \\[\n\\int_S dS = \\int_0^{2\\pi} du \\int_0^{\\pi} dv \\left|[-R\\sin u \\sin v,\\,R\\cos u\\sin v,\\,0]\\times[R\\cos u\\cos v,\\,R\\sin u\\cos v,-R\\sin v]\\right|\n\\] Pro-tip: take out as many factors from the cross product as you can before calculating it: \\[\n\\begin{aligned}\n\\int_S dS &= \\int_0^{2\\pi} du \\int_0^{\\pi} dv \\left|R^2 \\sin v\\right|\\cdot\\left|[-\\sin u,\\,\\cos u,\\,0]\\times[\\cos u\\cos v,\\,\\sin u\\cos v,-\\sin v]\\right| \\\\\n&= \\int_0^{2\\pi} du \\int_0^{\\pi} dv \\left|R^2 \\sin v\\right|\\cdot\\left|[-\\cos u \\sin v,\\,-\\sin u\\sin v,\\,-\\sin^2 u \\cos v -\\cos^2 u \\cos v]\\right| \\\\\n&= \\int_0^{2\\pi} du \\int_0^{\\pi} dv \\left|R^2 \\sin v\\right|\\cdot\\left|[\\cos u \\sin v,\\,\\sin u\\sin v,\\,\\cos v]\\right| \\\\\n&= \\int_0^{2\\pi} du \\int_0^{\\pi} dv \\left|R^2 \\sin v\\right|\n\\end{aligned}\n\\] where we used the fact that \\(\\left|[\\cos u \\sin v,\\,\\sin u\\sin v,\\,\\cos v]\\right| = 1\\) (this is good to remember).\nWe don’t need to worry about the absolute value of \\(\\sin v\\) because \\(v\\in[0,\\pi]\\) where sine is non-negative. There is no \\(u\\) in this expression so we can just mupltiply by \\(2\\pi\\) to give \\[\n\\begin{aligned}\n\\int_S dS = 2\\pi R^2 \\int_0^{\\pi} dv \\sin v = 2\\pi R^2 [-\\cos u]_0^{\\pi} = 4\\pi R^2.\n\\end{aligned}\n\\] But you knew that already.",
    "crumbs": [
      "Chapter 3: Integrals (Weeks 17/18)",
      "Surface integrals"
    ]
  },
  {
    "objectID": "volumeintegrals.html",
    "href": "volumeintegrals.html",
    "title": "Volume integrals",
    "section": "",
    "text": "In some ways these are the easiest integrals we see in this module. It’s basically the same steps as before:\n\nParameterise the volume.\nFind the element \\(dV\\) for that parameterisation.\nReplace the integral \\(\\int_V\\) with ordinary integrals \\(\\int_a^b\\) with the correct limits.\nCompute the value of the triple integral.\n\nHowever, the parameterisation of 3D volume needs 3 parameters, and we already know how to parameterise 3D space: that’s a co-ordinate system. So it’s a case of choosing the co-ordinate system which best describes the volume you’re interested in, and working out the correct limits. The element \\(dV\\) for a given co-ordinate system is the product of the scale factors.\nFor Cartestians, \\[\ndV = h_x h_y h_z \\,dx\\,dy\\,dz = dx\\,dy\\,dz,\n\\] for cylindrical polars, \\[\ndV = h_\\rho h_\\phi h_z \\,d\\rho\\,d\\phi\\,dz = \\rho d\\rho\\,d\\phi\\,dz,\n\\] and for spherical polars, \\[\ndV = h_r h_\\phi h_\\theta \\,dr\\,d\\phi\\,d\\theta = r^2\\sin \\theta \\,dr\\,d\\phi\\,d\\theta.\n\\]\n\nExample: volume of a sphere\nCalculate the volume of the sphere \\(x^2+y^2+z^2 \\leq R^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe volume is the empty integral \\[\n\\text{Volume} = \\int_V dV.\n\\]\nWe use spherical polar coordinates, where \\(x = r\\sin\\theta\\cos\\phi\\), \\(y = r\\sin\\theta\\sin\\phi\\), \\(z = r\\cos\\theta\\), and \\(dV = r^2 \\sin\\theta \\, dr\\, d\\theta\\, d\\phi\\).\nThe limits are \\(0 \\leq r \\leq R\\), \\(0 \\leq \\theta \\leq \\pi\\), \\(0 \\leq \\phi \\leq 2\\pi\\).\nSo, \\[\n\\text{Volume} = \\int_0^{2\\pi} d\\phi \\int_0^\\pi d\\theta dr\\,\\int_0^R r^2  \\sin\\theta = (2\\pi) \\left( \\int_0^\\pi \\sin\\theta\\, d\\theta \\right) \\left( \\int_0^R r^2 dr \\right).\n\\] Notice how we can do each of the integrals separately in this case.\n\\(\\int_0^\\pi \\sin\\theta\\, d\\theta = 2\\), and \\(\\int_0^R r^2 dr = \\frac{R^3}{3}\\).\nTherefore, the volume is \\(2\\pi \\times 2 \\times \\frac{R^3}{3} = \\frac{4}{3}\\pi R^3\\).\n\n\n\n\n\nExample\nCalculate the volume integral \\[\n\\int_V z^3 dV\n\\] where \\(V = \\{(x,y,z)\\in\\mathbb{R}\\,|\\,0\\leq z\\leq 1,\\,x^2+y^2\\leq z^2\\}\\).\nHint: this is a cone.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe set is given in Cartesian co-ordinates, but don’t let that fool you: it will get very messy if you try to do this in Cartesians.\nThe \\(x^2+y^2\\) should make you think of cylindrical co-ordinates, since \\(\\rho^2=x^2+y^2\\). Then you get \\[V = \\{(\\rho,\\phi,z)\\,|\\,0\\leq z\\leq 1,\\,\\rho\\leq z\\},\\] which is more obviously a cone (sketch in the \\(\\phi=0\\) plane).\nImplicit in this definition is that \\(\\phi\\) can take any value between \\(0\\) and \\(2\\pi\\) as usual.\nSo now we can put it all together: \\[\n\\int_V z^3 dV = \\int_0^z d\\rho \\int_0^{2\\pi} d\\phi \\int_0^1 dz z^3 \\rho,\n\\] with the extra \\(\\rho\\) coming from thee scale factor \\(h_\\phi\\).\nThe \\(\\phi\\) integral is easy because \\(\\phi\\) does not appear in the integrand. We need to be careful with the order of the other two, because the limits for \\(\\rho\\) depend on \\(z\\): \\[\n\\int_V z^3 dV = 2\\pi  \\int_0^1 z^3 \\left(\\int_0^z \\rho d\\rho\\right) dz = 2\\pi \\int_0^1 z^3\\, \\frac{1}{2} z^2 dz = \\frac{\\pi}{6}\n\\]",
    "crumbs": [
      "Chapter 3: Integrals (Weeks 17/18)",
      "Volume integrals"
    ]
  },
  {
    "objectID": "fourierseries.html#orthogonality-of-sines-and-cosines",
    "href": "fourierseries.html#orthogonality-of-sines-and-cosines",
    "title": "Fourier series",
    "section": "Orthogonality of sines and cosines",
    "text": "Orthogonality of sines and cosines\nFor two vectors \\({\\mathbf{u}}\\) and \\({\\mathbf{v}}\\), we say they are orthogonal if their dot product is zero, that is \\({\\mathbf{u}} \\cdot {\\mathbf{v}} = 0\\). This means that the two vectors are perpendicular to each other in some sense.\nWe can generalise the dot product to an inner product defined as an integral. For periodic functions \\(f(x)\\) and \\(g(x)\\) with period \\(L\\), we define the inner product as \\[\n\\langle f(x), g(x) \\rangle = \\int_0^L f(x) g(x) dx.\n\\] Note that this satisfies \\(\\langle f(x), g(x) + h(x) \\rangle = \\langle f(x), g(x) \\rangle + \\langle f(x), h(x) \\rangle\\), and \\(\\langle f(x), c g(x) \\rangle = c \\langle f(x), g(x) \\rangle\\) for any constant \\(c\\). We will use these rules later.\nTwo functions are called orthogonal if their inner product is zero, that is \\(\\langle f, g \\rangle = 0\\).\nSine and cosine are both periodic functions with period \\(2\\pi\\), and they are orthogonal because \\[\n\\langle \\cos x, \\sin x \\rangle = \\int_0^{2\\pi} \\cos x \\sin x \\,dx = \\int_0^{2\\pi} \\frac{1}{2} \\sin(2x) \\,dx = 0.\n\\]\nIn fact, for any period \\(L\\) and any integers \\(m\\) and \\(n\\), we have \\[\n\\begin{aligned}\n\\left\\langle \\cos\\left(\\frac{2\\pi m x}{L}\\right), \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right\\rangle &= \\int_0^L \\cos\\left(\\frac{2\\pi m x}{L}\\right) \\sin\\left(\\frac{2\\pi n x}{L}\\right) dx &= 0, \\\\\n\\left\\langle \\cos\\left(\\frac{2\\pi m x}{L}\\right), \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\right\\rangle &= \\int_0^L \\cos\\left(\\frac{2\\pi m x}{L}\\right) \\cos\\left(\\frac{2\\pi n x}{L}\\right) dx &= \\begin{cases}\nL & m = n = 0 \\\\\n\\frac{L}{2} & m = n &gt; 0 \\\\\n0 & m \\neq n\n\\end{cases},\\\\\n\\left\\langle \\sin\\left(\\frac{2\\pi m x}{L}\\right), \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right\\rangle &= \\int_0^L \\sin\\left(\\frac{2\\pi m x}{L}\\right) \\sin\\left(\\frac{2\\pi n x}{L}\\right) dx &= \\begin{cases}\n0 & m = 0 \\\\\n\\frac{L}{2} & m = n &gt; 0 \\\\\n0 & m \\neq n\n\\end{cases}\n\\end{aligned}\n\\]\nThis means that the set \\[\n\\left\\{ 1, \\cos\\left(\\frac{2\\pi x}{L}\\right), \\sin\\left(\\frac{2\\pi x}{L}\\right), \\cos\\left(\\frac{4\\pi x}{L}\\right), \\sin\\left(\\frac{4\\pi x}{L}\\right), \\ldots \\right\\}\n\\] is an orthogonal set of functions which are all periodic with period \\(L\\), and it is this property that allows us to find the Fourier coefficients using the inner product.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier Series"
    ]
  },
  {
    "objectID": "fourierseries.html#footnotes",
    "href": "fourierseries.html#footnotes",
    "title": "Fourier series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are various conventions for the Fourier series, but this is the one we will use in this module.↩︎",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier Series"
    ]
  },
  {
    "objectID": "pdes.html#odes-from-pdes",
    "href": "pdes.html#odes-from-pdes",
    "title": "Examples of PDEs",
    "section": "ODEs from PDEs",
    "text": "ODEs from PDEs\nPDEs, even linear ones, are often very difficult to solve directly. However, by making certain assumptions about the form of the solution, we can reduce a PDE to an ODE, which is much easier to solve. For example, in the heat equation \\({\\frac{\\partial u}{\\partial t}} = {\\frac{\\partial^2 u}{\\partial x^2}}\\), if we assume that the solution is independent of space, i.e. \\(u(x,t) = u(t)\\), then the PDE reduces to the ODE \\({\\frac{\\partial u}{\\partial t}} = 0\\), which has the solution \\(u(t) = C\\) for some constant \\(C\\). Similarly we could assume that the solution is independent of time, i.e. \\(u(x,t) = u(x)\\), which reduces the PDE to the ODE \\({\\frac{\\partial^2 u}{\\partial x^2}} = 0\\), which has the solution \\(u(x) = C_1 x + C_2\\) for some constants \\(C_1\\) and \\(C_2\\).\nBy making a very slightly more general assumption about the form of the solution, we arrive at the method of separation of variables, which we will see in the next section.\nIn all of these cases, we are just guessing a solution of a particular form, and then checking that it satisfies the PDE. This is a very common technique for solving PDEs, and it is often the first step in finding a solution to a PDE. But this means that it is very important to check whether the solution we have found is unique, and whether it satisfies the boundary and initial conditions that we have specified.",
    "crumbs": [
      "Chapter 5: ODEs and PDEs (Week 20)",
      "Examples of PDEs"
    ]
  },
  {
    "objectID": "fourierdes.html",
    "href": "fourierdes.html",
    "title": "Fourier series for ODEs and PDEs",
    "section": "",
    "text": "Our main reason for introducing Fourier series in this module is to use them to solve differential equations. Firstly, Fourier series arise automatically when we use separation of variables.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Application to ODEs and PDEs"
    ]
  },
  {
    "objectID": "fourierdes.html#fourier-series-and-differentiation",
    "href": "fourierdes.html#fourier-series-and-differentiation",
    "title": "Fourier series for ODEs and PDEs",
    "section": "Fourier series and differentiation",
    "text": "Fourier series and differentiation\nMore generally, we can use Fourier series to solve ODEs and PDEs by writing the solution as a Fourier series, and then using the properties of Fourier series to turn the differential equation into an algebraic equation for the Fourier coefficients.\nWe can do this because of one important property. If \\[\nf(x) = a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] then we can differentiate this term by term, to get \\[\nf'(x) = \\sum_{n=1}^\\infty \\left[ -\\frac{2\\pi n}{L} a_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) + \\frac{2\\pi n}{L} b_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] and therefore the Fourier coefficients of \\(f'\\) are \\[\n\\begin{aligned}\nc_n &= \\frac{2\\pi n}{L} b_n, \\\\\nd_n &= -\\frac{2\\pi n}{L} a_n.\n\\end{aligned}\n\\]\nSimilarly, the second derivative of \\(f\\) is \\[\nf''(x) = \\sum_{n=1}^\\infty \\left[ -\\left(\\frac{2\\pi n}{L}\\right)^2 a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) - \\left(\\frac{2\\pi n}{L}\\right)^2 b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] and so the Fourier coefficients of \\(f''\\) are the same as those of \\(f\\), but multiplied by \\(-\\left(\\frac{2\\pi n}{L}\\right)^2\\).\n\nExample (ODE)\nConsider the equation \\[\nf''(x) + f(x) = g(x),\n\\] where \\(g(x)\\) is a known function, and we are looking for a solution \\(f(x)\\).\nWe can write \\(g(x)\\) as a Fourier series: \\[\ng(x) = c_0 + \\sum_{n=1}^\\infty \\left[ c_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + d_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\]\nIf we write \\(f(x)\\) as a Fourier series: \\[\nf(x) = a_0 +\\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] then the second derivative of \\(f\\) is: \\[\nf''(x) = \\sum_{n=1}^\\infty \\left[ -\\left(\\frac{2\\pi n}{L}\\right)^2 a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) - \\left(\\frac{2\\pi n}{L}\\right)^2 b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\]\nSubstituting into the differential equation, we get: \\[\n\\begin{aligned}\n&\\sum_{n=1}^\\infty \\left[ -\\left(\\frac{2\\pi n}{L}\\right)^2 a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) - \\left(\\frac{2\\pi n}{L}\\right)^2 b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right] + a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right] \\\\&= c_0 + \\sum_{n=1}^\\infty \\left[ c_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + d_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\end{aligned}\n\\] Because of the orthogonality of sines and cosines, we can compare coefficients to get different equations for the Fourier coefficients: \\[\n\\begin{aligned}\na_0 &= c_0, \\\\\n\\left(1 - \\left(\\frac{2\\pi n}{L}\\right)^2\\right) a_n &= c_n, \\\\\n\\left(1 - \\left(\\frac{2\\pi n}{L}\\right)^2\\right) b_n &= d_n.\n\\end{aligned}\n\\] If we know the Fourier coefficients of \\(g\\), then we can solve for the Fourier coefficients of \\(f\\), and hence find \\(f\\) itself.\nFor example, if \\(g(x) = \\cos\\left(\\frac{2\\pi m x}{L}\\right)\\) for some integer \\(m\\), then the Fourier coefficients of \\(g\\) are \\(c_m = 1\\) and all the other coefficients are zero. In this case, we have \\[\n\\begin{aligned}\na_0 &= 0, \\\\\na_n &= \\frac{1}{1 - \\left(\\frac{2\\pi n}{L}\\right)^2} \\quad\\text{for } n = m, \\\\\na_n &= 0 \\quad\\text{for } n \\neq m, \\\\\nb_n &= 0 \\quad\\text{for all } n,\n\\end{aligned}\n\\] which then tells us that \\[\nf(x) = \\frac{\\cos\\left(\\frac{2\\pi m x}{L}\\right)}{1 - \\left(\\frac{2\\pi m}{L}\\right)^2}.\n\\]\nSubstituting this back into the original equation, we can check that it is indeed a solution: \\[\n\\begin{aligned}\nf''(x) + f(x) &= -\\left(\\frac{2\\pi m}{L}\\right)^2 \\frac{\\cos\\left(\\frac{2\\pi m x}{L}\\right)}{1 - \\left(\\frac{2\\pi m}{L}\\right)^2} + \\frac{\\cos\\left(\\frac{2\\pi m x}{L}\\right)}{1 - \\left(\\frac{2\\pi m}{L}\\right)^2} \\\\&= \\cos\\left(\\frac{2\\pi m x}{L}\\right).\n\\end{aligned}\n\\]\n\n\nExample (PDE)\nConsider the hyperdiffusion equation \\[\n{\\frac{\\partial u}{\\partial t}} = -\\alpha {\\frac{\\partial^4 u}{\\partial x^4}},\n\\] where \\(\\alpha\\) is a positive constant. Let’s look for a solution \\(u(x,t)\\) and the initial condition \\(u(x,0) = f(x)\\) for some periodic function \\(f\\) with period \\(L\\).\nWe could use separation of variables to solve this, but it is easier to write \\(u\\) as a Fourier series (assuming the solution is always periodic with period \\(L\\)), and then use the properties of Fourier series to turn the PDE into an ODE for the Fourier coefficients.\nWe write \\[\nu(x,t) = a_0(t) + \\sum_{n=1}^\\infty \\left[ a_n(t) \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n(t) \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\] Notice that the Fourier coefficients are now functions of \\(t\\), because \\(u\\) is a function of both \\(x\\) and \\(t\\).\nThen the fourth derivative of \\(u\\) with respect to \\(x\\) is \\[\n{\\frac{\\partial^4 u}{\\partial x^4}} = \\sum_{n=1}^\\infty \\left[ \\left(\\frac{2\\pi n}{L}\\right)^4 a_n(t) \\cos\\left(\\frac{2\\pi n x}{L}\\right) + \\left(\\frac{2\\pi n}{L}\\right)^4 b_n(t) \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\]\nSubstituting into the PDE and comparing coefficients of sines and cosines, we get \\[\n\\begin{aligned}\n\\frac{da_0}{dt} &= 0, \\\\\n\\frac{da_n}{dt} &= -\\alpha \\left(\\frac{2\\pi n}{L}\\right)^4 a_n, \\\\\n\\frac{db_n}{dt} &= -\\alpha \\left(\\frac{2\\pi n}{L}\\right)^4 b_n,\n\\end{aligned}\n\\] which is a family of ODEs for the Fourier coefficients. We can solve these ODEs to get \\[\n\\begin{aligned}\na_0(t) &= a_0(0), \\\\\na_n(t) &= a_n(0) e^{-\\alpha \\left(\\frac{2\\pi n}{L}\\right)^4 t}, \\\\\nb_n(t) &= b_n(0) e^{-\\alpha \\left(\\frac{2\\pi n}{L}\\right)^4 t}.\n\\end{aligned}\n\\]\nTo satisfy the initial condition \\(u(x,0) = f(x)\\), we need to choose the Fourier coefficients at time \\(t=0\\) such that \\[\nf(x) = a_0(0) + \\sum_{n=1}^\\infty \\left[ a_n(0) \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n(0) \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right].\n\\] This is exactly a Fourier series, so we can use the Fourier coefficients of \\(f\\) to find the initial values of the Fourier coefficients of \\(u\\), and hence find \\(u\\) itself.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Application to ODEs and PDEs"
    ]
  },
  {
    "objectID": "fourierstheorem.html",
    "href": "fourierstheorem.html",
    "title": "Theorems about Fourier series",
    "section": "",
    "text": "Fourier analysis (both series and transforms, which we will see in the next section) is a vast area of mathematics, and there are many theorems. We will only cover two of the most important ones here. We will later see versions of these theorems for Fourier transforms.\nAgain, we use the convention that \\[\nf(x) = a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right],\n\\] where \\(L\\) is the period of the function \\(f\\).",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier series theorems"
    ]
  },
  {
    "objectID": "fourierstheorem.html#convolution-theorem",
    "href": "fourierstheorem.html#convolution-theorem",
    "title": "Theorems about Fourier series",
    "section": "Convolution theorem",
    "text": "Convolution theorem\nConsider two periodic functions \\(f(x)\\) and \\(g(x)\\), with the same period \\(L\\), and with Fourier coefficients \\(a_n, b_n\\) and \\(c_n, d_n\\) respectively. We can define the convolution of these two functions as \\[\n(f * g)(x) = \\int_0^L f(y) g(x - y) \\, dy.\n\\] Here \\(f * g\\) is a new function, which is also periodic with period \\(L\\). Functions like this arise in many contexts, such as signal processing, probability theory, and the study of PDEs. Versions of this are particularly important in modern machine learning, where they are used in convolutional neural networks (CNNs).\nThe convolution theorem states that the Fourier coefficients \\(A_n\\) and \\(B_n\\) of the function \\(f * g\\) are given by \\[\n\\begin{aligned}\nA_n &= a_n c_n - b_n d_n, \\\\\nB_n &= a_n d_n + b_n c_n.\n\\end{aligned}\n\\]\n\nProof (Non-examinable)\nThis can be calculated directly, just by substituting the Fourier series for \\(f\\) and \\(g\\) into the definition of the convolution, and then using the orthogonality of sines and cosines to evaluate the integrals. The details are a bit tedious, but it is a straightforward calculation.\n\\[\n\\begin{aligned}\nA_0 &=\\frac{1}{L} \\int_0^L (f * g)(x) \\, dx \\\\&= \\frac{1}{L} \\int_0^L \\left( \\int_0^L f(y) g(x - y) \\, dy \\right) dx \\\\\n&= \\int_0^L f(y) \\left( \\frac{1}{L} \\int_0^L g(x - y) \\, dx \\right) dy \\\\\n&= \\int_0^L f(y) c_0 \\, dy \\\\\n&= c_0 a_0.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier series theorems"
    ]
  },
  {
    "objectID": "fourierstheorem.html#parsevals-theorem",
    "href": "fourierstheorem.html#parsevals-theorem",
    "title": "Theorems about Fourier series",
    "section": "Parseval’s theorem",
    "text": "Parseval’s theorem\nParseval’s theorem states that the Fourier coefficients of a function \\(f\\) satisfy \\[\n\\frac{1}{L} \\int_0^L |f(x)|^2\n= a_0^2 + \\frac{1}{2} \\sum_{n=1}^\\infty (a_n^2 + b_n^2).\n\\] This is a very important result, because it tells us that the Fourier coefficients contain all the information about the function \\(f\\). In particular, it tells us that the sum of the squares of the Fourier coefficients is equal to the average value of the square of the function. This is a kind of “energy conservation” principle, which is very useful in many applications.\n\nProof (Non-examinable)\nHere we can again use the inner product. \\[\n\\begin{aligned}\n\\frac{1}{L} \\int_0^L |f(x)|^2 dx &= \\frac{1}{L} \\langle f(x), f(x) \\rangle \\\\\n&= \\frac{1}{L} \\left\\langle a_0 + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{2\\pi n x}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\right], a_0 + \\sum_{m=1}^\\infty \\left[ a_m \\cos\\left(\\frac{2\\pi m x}{L}\\right) + b_m \\sin\\left(\\frac{2\\pi m x}{L}\\right) \\right] \\right\\rangle \\\\\n&= \\frac{a_0^2}{L} \\langle 1, 1 \\rangle + \\sum_{n=1}^\\infty \\left[ \\frac{a_n^2}{L} \\langle \\cos\\left(\\frac{2\\pi n x}{L}\\right), \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\rangle + \\frac{b_n^2}{L} \\langle \\sin\\left(\\frac{2\\pi n x}{L}\\right), \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\rangle \\right] \\\\&= a_0^2 + \\frac{1}{2} \\sum_{n=1}^\\infty (a_n^2 + b_n^2).\n\\end{aligned}\n\\] where we have used the facts that \\(\\langle 1, 1 \\rangle = L\\), \\(\\langle \\cos\\left(\\frac{2\\pi n x}{L}\\right), \\cos\\left(\\frac{2\\pi n x}{L}\\right) \\rangle = \\frac{L}{2}\\), and \\(\\langle \\sin\\left(\\frac{2\\pi n x}{L}\\right), \\sin\\left(\\frac{2\\pi n x}{L}\\right) \\rangle = \\frac{L}{2}\\), with all the other inner products being zero.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier series theorems"
    ]
  },
  {
    "objectID": "fourierstheorem.html#convolution-theorem-non-examinable",
    "href": "fourierstheorem.html#convolution-theorem-non-examinable",
    "title": "Theorems about Fourier series",
    "section": "Convolution theorem (Non-examinable)",
    "text": "Convolution theorem (Non-examinable)\nConsider two periodic functions \\(f(x)\\) and \\(g(x)\\), with the same period \\(L\\), and with Fourier coefficients \\(a_n, b_n\\) and \\(c_n, d_n\\) respectively. We can define the convolution of these two functions as \\[\n(f * g)(x) = \\frac{1}{L}\\int_0^L f(y) g(x - y) \\, dy.\n\\] Here \\(f * g\\) is a new function, which is also periodic with period \\(L\\). Functions like this arise in many contexts, such as signal processing, probability theory, and the study of PDEs. Versions of this are particularly important in modern machine learning, where they are used in convolutional neural networks (CNNs).\nThe convolution theorem states that the Fourier coefficients \\(A_n\\) and \\(B_n\\) of the function \\(f * g\\) are given by \\[\n\\begin{aligned}\nA_n &= a_n c_n - b_n d_n, \\\\\nB_n &= a_n d_n + b_n c_n.\n\\end{aligned}\n\\]\nThis can be proved using the definition of the convolution and the Fourier coefficient formulas.",
    "crumbs": [
      "Chapter 6: Fourier Series (Week 21)",
      "Fourier series theorems"
    ]
  }
]